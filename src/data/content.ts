// This file is auto-generated. Do not edit manually.
// Generated at: 2025-09-13T17:05:15.471Z

export interface BlogPost {
  slug: string;
  title: string;
  excerpt: string;
  content: string;
  contentHtml: string;
  author: string;
  date: string;
  readTime: number;
  category: string;
  tags: string[];
  image: string;
  featured: boolean;
}

export interface ProjectPost {
  slug: string;
  title: string;
  excerpt: string;
  content: string;
  contentHtml: string;
  image: string;
  technologies: string[];
  githubUrl: string;
  liveUrl: string;
  date: string;
  category: string;
  featured: boolean;
}

export const blogPosts: BlogPost[] = [
  {
    "slug": "2024-01-15-scalable-microservices",
    "title": "Building Scalable Microservices with Node.js and Docker",
    "excerpt": "A comprehensive guide to architecting microservices that can handle millions of requests while maintaining code quality and deployment simplicity.",
    "content": "\n# Building Scalable Microservices with Node.js and Docker\n\n## Introduction\n\nMicroservices architecture has become the gold standard for building scalable, maintainable applications that can handle millions of users. In this comprehensive guide, we'll explore how to design, implement, and deploy microservices using Node.js and Docker, focusing on real-world scalability challenges and solutions.\n\n## Why Microservices?\n\nTraditional monolithic applications face several challenges as they grow:\n\n- **Scaling bottlenecks**: The entire application must be scaled, even if only one component needs more resources\n- **Technology lock-in**: The entire application is tied to a single technology stack\n- **Deployment risks**: A bug in one feature can bring down the entire application\n- **Team coordination**: Large teams working on the same codebase leads to conflicts and slower development\n\nMicroservices solve these problems by breaking applications into smaller, independent services that can be developed, deployed, and scaled independently.\n\n## Architecture Principles\n\n### 1. Single Responsibility Principle\n\nEach microservice should have one clear business purpose. For example:\n\n```javascript\n// User Service - handles user management\nconst userService = {\n  createUser: async (userData) => { /* ... */ },\n  getUserById: async (userId) => { /* ... */ },\n  updateUser: async (userId, updates) => { /* ... */ },\n  deleteUser: async (userId) => { /* ... */ }\n};\n\n// Order Service - handles order processing\nconst orderService = {\n  createOrder: async (orderData) => { /* ... */ },\n  getOrderById: async (orderId) => { /* ... */ },\n  updateOrderStatus: async (orderId, status) => { /* ... */ },\n  cancelOrder: async (orderId) => { /* ... */ }\n};\n```\n\n### 2. Database Per Service\n\nEach microservice should own its data and database schema:\n\n```javascript\n// User Service Database Schema\nconst userSchema = {\n  id: 'UUID',\n  email: 'STRING',\n  password: 'HASHED_STRING',\n  profile: 'JSON',\n  createdAt: 'TIMESTAMP',\n  updatedAt: 'TIMESTAMP'\n};\n\n// Order Service Database Schema\nconst orderSchema = {\n  id: 'UUID',\n  userId: 'UUID', // Reference to user, but no foreign key constraint\n  items: 'JSON',\n  status: 'ENUM',\n  totalAmount: 'DECIMAL',\n  createdAt: 'TIMESTAMP'\n};\n```\n\n### 3. API-First Communication\n\nServices communicate exclusively through well-defined APIs:\n\n```javascript\n// Inter-service communication example\nclass OrderService {\n  async createOrder(orderData) {\n    // Validate user exists by calling User Service API\n    const user = await this.userServiceClient.getUserById(orderData.userId);\n    if (!user) {\n      throw new Error('User not found');\n    }\n\n    // Create order\n    const order = await this.orderRepository.create(orderData);\n    \n    // Publish order created event\n    await this.eventBus.publish('order.created', {\n      orderId: order.id,\n      userId: order.userId,\n      amount: order.totalAmount\n    });\n\n    return order;\n  }\n}\n```\n\n## Implementation with Node.js\n\n### Service Structure\n\nHere's a recommended structure for a Node.js microservice:\n\n```\nuser-service/\n├── src/\n│   ├── controllers/\n│   │   └── userController.js\n│   ├── services/\n│   │   └── userService.js\n│   ├── repositories/\n│   │   └── userRepository.js\n│   ├── models/\n│   │   └── user.js\n│   ├── middleware/\n│   │   ├── auth.js\n│   │   ├── validation.js\n│   │   └── errorHandler.js\n│   ├── routes/\n│   │   └── userRoutes.js\n│   ├── config/\n│   │   ├── database.js\n│   │   └── environment.js\n│   └── app.js\n├── tests/\n├── Dockerfile\n├── docker-compose.yml\n├── package.json\n└── README.md\n```\n\n### Example Service Implementation\n\n```javascript\n// src/app.js\nconst express = require('express');\nconst helmet = require('helmet');\nconst cors = require('cors');\nconst rateLimit = require('express-rate-limit');\nconst userRoutes = require('./routes/userRoutes');\nconst errorHandler = require('./middleware/errorHandler');\n\nconst app = express();\n\n// Security middleware\napp.use(helmet());\napp.use(cors());\n\n// Rate limiting\nconst limiter = rateLimit({\n  windowMs: 15 * 60 * 1000, // 15 minutes\n  max: 100 // limit each IP to 100 requests per windowMs\n});\napp.use(limiter);\n\n// Body parsing\napp.use(express.json({ limit: '10mb' }));\napp.use(express.urlencoded({ extended: true }));\n\n// Health check endpoint\napp.get('/health', (req, res) => {\n  res.json({\n    status: 'healthy',\n    timestamp: new Date().toISOString(),\n    uptime: process.uptime(),\n    memory: process.memoryUsage()\n  });\n});\n\n// API routes\napp.use('/api/v1/users', userRoutes);\n\n// Error handling\napp.use(errorHandler);\n\nmodule.exports = app;\n```\n\n```javascript\n// src/controllers/userController.js\nconst userService = require('../services/userService');\nconst { validationResult } = require('express-validator');\n\nclass UserController {\n  async createUser(req, res, next) {\n    try {\n      const errors = validationResult(req);\n      if (!errors.isEmpty()) {\n        return res.status(400).json({ errors: errors.array() });\n      }\n\n      const user = await userService.createUser(req.body);\n      res.status(201).json({\n        success: true,\n        data: user,\n        message: 'User created successfully'\n      });\n    } catch (error) {\n      next(error);\n    }\n  }\n\n  async getUserById(req, res, next) {\n    try {\n      const { id } = req.params;\n      const user = await userService.getUserById(id);\n      \n      if (!user) {\n        return res.status(404).json({\n          success: false,\n          message: 'User not found'\n        });\n      }\n\n      res.json({\n        success: true,\n        data: user\n      });\n    } catch (error) {\n      next(error);\n    }\n  }\n}\n\nmodule.exports = new UserController();\n```\n\n## Docker Configuration\n\n### Dockerfile\n\n```dockerfile\n# Multi-stage build for production optimization\nFROM node:18-alpine AS builder\n\nWORKDIR /app\nCOPY package*.json ./\nRUN npm ci --only=production && npm cache clean --force\n\nFROM node:18-alpine AS runtime\n\n# Create non-root user for security\nRUN addgroup -g 1001 -S nodejs\nRUN adduser -S nodeuser -u 1001\n\nWORKDIR /app\n\n# Copy dependencies from builder stage\nCOPY --from=builder /app/node_modules ./node_modules\nCOPY --chown=nodeuser:nodejs . .\n\n# Switch to non-root user\nUSER nodeuser\n\n# Expose port\nEXPOSE 3000\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \\\n  CMD node healthcheck.js\n\n# Start the application\nCMD [\"node\", \"src/server.js\"]\n```\n\n### Docker Compose for Development\n\n```yaml\n# docker-compose.yml\nversion: '3.8'\n\nservices:\n  user-service:\n    build: .\n    ports:\n      - \"3001:3000\"\n    environment:\n      - NODE_ENV=development\n      - DATABASE_URL=postgresql://user:password@postgres:5432/userdb\n      - REDIS_URL=redis://redis:6379\n    depends_on:\n      - postgres\n      - redis\n    volumes:\n      - .:/app\n      - /app/node_modules\n    command: npm run dev\n\n  order-service:\n    build: ../order-service\n    ports:\n      - \"3002:3000\"\n    environment:\n      - NODE_ENV=development\n      - DATABASE_URL=postgresql://user:password@postgres:5432/orderdb\n      - USER_SERVICE_URL=http://user-service:3000\n    depends_on:\n      - postgres\n      - user-service\n\n  postgres:\n    image: postgres:15-alpine\n    environment:\n      - POSTGRES_USER=user\n      - POSTGRES_PASSWORD=password\n      - POSTGRES_DB=microservices\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n\n  redis:\n    image: redis:7-alpine\n    ports:\n      - \"6379:6379\"\n    volumes:\n      - redis_data:/data\n\n  nginx:\n    image: nginx:alpine\n    ports:\n      - \"80:80\"\n    volumes:\n      - ./nginx.conf:/etc/nginx/nginx.conf\n    depends_on:\n      - user-service\n      - order-service\n\nvolumes:\n  postgres_data:\n  redis_data:\n```\n\n## Scaling Strategies\n\n### Horizontal Scaling\n\n```yaml\n# docker-compose.prod.yml\nversion: '3.8'\n\nservices:\n  user-service:\n    build: .\n    deploy:\n      replicas: 3\n      resources:\n        limits:\n          cpus: '0.5'\n          memory: 512M\n        reservations:\n          cpus: '0.25'\n          memory: 256M\n      restart_policy:\n        condition: on-failure\n        delay: 5s\n        max_attempts: 3\n    environment:\n      - NODE_ENV=production\n      - DATABASE_URL=${DATABASE_URL}\n      - REDIS_URL=${REDIS_URL}\n```\n\n### Load Balancing with Nginx\n\n```nginx\n# nginx.conf\nupstream user_service {\n    least_conn;\n    server user-service-1:3000;\n    server user-service-2:3000;\n    server user-service-3:3000;\n}\n\nupstream order_service {\n    least_conn;\n    server order-service-1:3000;\n    server order-service-2:3000;\n}\n\nserver {\n    listen 80;\n    \n    location /api/v1/users {\n        proxy_pass http://user_service;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        \n        # Health check\n        proxy_next_upstream error timeout invalid_header http_500 http_502 http_503;\n        proxy_connect_timeout 5s;\n        proxy_send_timeout 10s;\n        proxy_read_timeout 10s;\n    }\n    \n    location /api/v1/orders {\n        proxy_pass http://order_service;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n    }\n}\n```\n\n## Monitoring and Observability\n\n### Application Metrics\n\n```javascript\n// src/middleware/metrics.js\nconst prometheus = require('prom-client');\n\n// Create metrics\nconst httpRequestDuration = new prometheus.Histogram({\n  name: 'http_request_duration_seconds',\n  help: 'Duration of HTTP requests in seconds',\n  labelNames: ['method', 'route', 'status_code']\n});\n\nconst httpRequestTotal = new prometheus.Counter({\n  name: 'http_requests_total',\n  help: 'Total number of HTTP requests',\n  labelNames: ['method', 'route', 'status_code']\n});\n\nconst activeConnections = new prometheus.Gauge({\n  name: 'active_connections',\n  help: 'Number of active connections'\n});\n\n// Middleware to collect metrics\nconst metricsMiddleware = (req, res, next) => {\n  const start = Date.now();\n  \n  res.on('finish', () => {\n    const duration = (Date.now() - start) / 1000;\n    const route = req.route ? req.route.path : req.path;\n    \n    httpRequestDuration\n      .labels(req.method, route, res.statusCode)\n      .observe(duration);\n    \n    httpRequestTotal\n      .labels(req.method, route, res.statusCode)\n      .inc();\n  });\n  \n  next();\n};\n\nmodule.exports = { metricsMiddleware, prometheus };\n```\n\n### Structured Logging\n\n```javascript\n// src/utils/logger.js\nconst winston = require('winston');\n\nconst logger = winston.createLogger({\n  level: process.env.LOG_LEVEL || 'info',\n  format: winston.format.combine(\n    winston.format.timestamp(),\n    winston.format.errors({ stack: true }),\n    winston.format.json()\n  ),\n  defaultMeta: {\n    service: process.env.SERVICE_NAME || 'microservice',\n    version: process.env.SERVICE_VERSION || '1.0.0'\n  },\n  transports: [\n    new winston.transports.File({ filename: 'logs/error.log', level: 'error' }),\n    new winston.transports.File({ filename: 'logs/combined.log' }),\n    new winston.transports.Console({\n      format: winston.format.combine(\n        winston.format.colorize(),\n        winston.format.simple()\n      )\n    })\n  ]\n});\n\nmodule.exports = logger;\n```\n\n## Best Practices\n\n### 1. Circuit Breaker Pattern\n\n```javascript\n// src/utils/circuitBreaker.js\nclass CircuitBreaker {\n  constructor(request, options = {}) {\n    this.request = request;\n    this.state = 'CLOSED';\n    this.failureCount = 0;\n    this.successCount = 0;\n    this.nextAttempt = Date.now();\n    \n    this.failureThreshold = options.failureThreshold || 5;\n    this.successThreshold = options.successThreshold || 2;\n    this.timeout = options.timeout || 60000;\n  }\n\n  async call(...args) {\n    if (this.state === 'OPEN') {\n      if (this.nextAttempt <= Date.now()) {\n        this.state = 'HALF_OPEN';\n      } else {\n        throw new Error('Circuit breaker is OPEN');\n      }\n    }\n\n    try {\n      const result = await this.request(...args);\n      return this.onSuccess(result);\n    } catch (error) {\n      return this.onFailure(error);\n    }\n  }\n\n  onSuccess(result) {\n    this.failureCount = 0;\n    \n    if (this.state === 'HALF_OPEN') {\n      this.successCount++;\n      if (this.successCount >= this.successThreshold) {\n        this.state = 'CLOSED';\n        this.successCount = 0;\n      }\n    }\n    \n    return result;\n  }\n\n  onFailure(error) {\n    this.failureCount++;\n    \n    if (this.failureCount >= this.failureThreshold) {\n      this.state = 'OPEN';\n      this.nextAttempt = Date.now() + this.timeout;\n    }\n    \n    throw error;\n  }\n}\n\nmodule.exports = CircuitBreaker;\n```\n\n### 2. Graceful Shutdown\n\n```javascript\n// src/server.js\nconst app = require('./app');\nconst logger = require('./utils/logger');\n\nconst PORT = process.env.PORT || 3000;\nconst server = app.listen(PORT, () => {\n  logger.info(`Server running on port ${PORT}`);\n});\n\n// Graceful shutdown\nprocess.on('SIGTERM', gracefulShutdown);\nprocess.on('SIGINT', gracefulShutdown);\n\nfunction gracefulShutdown(signal) {\n  logger.info(`Received ${signal}. Starting graceful shutdown...`);\n  \n  server.close((err) => {\n    if (err) {\n      logger.error('Error during server shutdown:', err);\n      process.exit(1);\n    }\n    \n    logger.info('Server closed successfully');\n    process.exit(0);\n  });\n  \n  // Force shutdown after 30 seconds\n  setTimeout(() => {\n    logger.error('Forced shutdown after timeout');\n    process.exit(1);\n  }, 30000);\n}\n```\n\n## Deployment to Production\n\n### Kubernetes Deployment\n\n```yaml\n# k8s/user-service-deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: user-service\n  labels:\n    app: user-service\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: user-service\n  template:\n    metadata:\n      labels:\n        app: user-service\n    spec:\n      containers:\n      - name: user-service\n        image: your-registry/user-service:latest\n        ports:\n        - containerPort: 3000\n        env:\n        - name: NODE_ENV\n          value: \"production\"\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: database-secret\n              key: url\n        resources:\n          requests:\n            memory: \"256Mi\"\n            cpu: \"250m\"\n          limits:\n            memory: \"512Mi\"\n            cpu: \"500m\"\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 3000\n          initialDelaySeconds: 30\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 3000\n          initialDelaySeconds: 5\n          periodSeconds: 5\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: user-service\nspec:\n  selector:\n    app: user-service\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 3000\n  type: ClusterIP\n```\n\n## Conclusion\n\nBuilding scalable microservices with Node.js and Docker requires careful attention to architecture, implementation, and operational concerns. Key takeaways:\n\n1. **Design for failure**: Implement circuit breakers, retries, and graceful degradation\n2. **Monitor everything**: Use structured logging, metrics, and distributed tracing\n3. **Automate deployment**: Use CI/CD pipelines and infrastructure as code\n4. **Plan for scale**: Design services to be stateless and horizontally scalable\n5. **Security first**: Implement authentication, authorization, and secure communication\n\nBy following these patterns and practices, you can build microservices that scale to handle millions of requests while maintaining reliability and performance.\n\nThe complete source code for this example is available on [GitHub](https://github.com/aayushpathak/scalable-microservices-example).",
    "contentHtml": "<h1>Building Scalable Microservices with Node.js and Docker</h1>\n<h2>Introduction</h2>\n<p>Microservices architecture has become the gold standard for building scalable, maintainable applications that can handle millions of users. In this comprehensive guide, we'll explore how to design, implement, and deploy microservices using Node.js and Docker, focusing on real-world scalability challenges and solutions.</p>\n<h2>Why Microservices?</h2>\n<p>Traditional monolithic applications face several challenges as they grow:</p>\n<ul>\n<li><strong>Scaling bottlenecks</strong>: The entire application must be scaled, even if only one component needs more resources</li>\n<li><strong>Technology lock-in</strong>: The entire application is tied to a single technology stack</li>\n<li><strong>Deployment risks</strong>: A bug in one feature can bring down the entire application</li>\n<li><strong>Team coordination</strong>: Large teams working on the same codebase leads to conflicts and slower development</li>\n</ul>\n<p>Microservices solve these problems by breaking applications into smaller, independent services that can be developed, deployed, and scaled independently.</p>\n<h2>Architecture Principles</h2>\n<h3>1. Single Responsibility Principle</h3>\n<p>Each microservice should have one clear business purpose. For example:</p>\n<pre><code class=\"language-javascript\">// User Service - handles user management\nconst userService = {\n  createUser: async (userData) => { /* ... */ },\n  getUserById: async (userId) => { /* ... */ },\n  updateUser: async (userId, updates) => { /* ... */ },\n  deleteUser: async (userId) => { /* ... */ }\n};\n\n// Order Service - handles order processing\nconst orderService = {\n  createOrder: async (orderData) => { /* ... */ },\n  getOrderById: async (orderId) => { /* ... */ },\n  updateOrderStatus: async (orderId, status) => { /* ... */ },\n  cancelOrder: async (orderId) => { /* ... */ }\n};\n</code></pre>\n<h3>2. Database Per Service</h3>\n<p>Each microservice should own its data and database schema:</p>\n<pre><code class=\"language-javascript\">// User Service Database Schema\nconst userSchema = {\n  id: 'UUID',\n  email: 'STRING',\n  password: 'HASHED_STRING',\n  profile: 'JSON',\n  createdAt: 'TIMESTAMP',\n  updatedAt: 'TIMESTAMP'\n};\n\n// Order Service Database Schema\nconst orderSchema = {\n  id: 'UUID',\n  userId: 'UUID', // Reference to user, but no foreign key constraint\n  items: 'JSON',\n  status: 'ENUM',\n  totalAmount: 'DECIMAL',\n  createdAt: 'TIMESTAMP'\n};\n</code></pre>\n<h3>3. API-First Communication</h3>\n<p>Services communicate exclusively through well-defined APIs:</p>\n<pre><code class=\"language-javascript\">// Inter-service communication example\nclass OrderService {\n  async createOrder(orderData) {\n    // Validate user exists by calling User Service API\n    const user = await this.userServiceClient.getUserById(orderData.userId);\n    if (!user) {\n      throw new Error('User not found');\n    }\n\n    // Create order\n    const order = await this.orderRepository.create(orderData);\n    \n    // Publish order created event\n    await this.eventBus.publish('order.created', {\n      orderId: order.id,\n      userId: order.userId,\n      amount: order.totalAmount\n    });\n\n    return order;\n  }\n}\n</code></pre>\n<h2>Implementation with Node.js</h2>\n<h3>Service Structure</h3>\n<p>Here's a recommended structure for a Node.js microservice:</p>\n<pre><code>user-service/\n├── src/\n│   ├── controllers/\n│   │   └── userController.js\n│   ├── services/\n│   │   └── userService.js\n│   ├── repositories/\n│   │   └── userRepository.js\n│   ├── models/\n│   │   └── user.js\n│   ├── middleware/\n│   │   ├── auth.js\n│   │   ├── validation.js\n│   │   └── errorHandler.js\n│   ├── routes/\n│   │   └── userRoutes.js\n│   ├── config/\n│   │   ├── database.js\n│   │   └── environment.js\n│   └── app.js\n├── tests/\n├── Dockerfile\n├── docker-compose.yml\n├── package.json\n└── README.md\n</code></pre>\n<h3>Example Service Implementation</h3>\n<pre><code class=\"language-javascript\">// src/app.js\nconst express = require('express');\nconst helmet = require('helmet');\nconst cors = require('cors');\nconst rateLimit = require('express-rate-limit');\nconst userRoutes = require('./routes/userRoutes');\nconst errorHandler = require('./middleware/errorHandler');\n\nconst app = express();\n\n// Security middleware\napp.use(helmet());\napp.use(cors());\n\n// Rate limiting\nconst limiter = rateLimit({\n  windowMs: 15 * 60 * 1000, // 15 minutes\n  max: 100 // limit each IP to 100 requests per windowMs\n});\napp.use(limiter);\n\n// Body parsing\napp.use(express.json({ limit: '10mb' }));\napp.use(express.urlencoded({ extended: true }));\n\n// Health check endpoint\napp.get('/health', (req, res) => {\n  res.json({\n    status: 'healthy',\n    timestamp: new Date().toISOString(),\n    uptime: process.uptime(),\n    memory: process.memoryUsage()\n  });\n});\n\n// API routes\napp.use('/api/v1/users', userRoutes);\n\n// Error handling\napp.use(errorHandler);\n\nmodule.exports = app;\n</code></pre>\n<pre><code class=\"language-javascript\">// src/controllers/userController.js\nconst userService = require('../services/userService');\nconst { validationResult } = require('express-validator');\n\nclass UserController {\n  async createUser(req, res, next) {\n    try {\n      const errors = validationResult(req);\n      if (!errors.isEmpty()) {\n        return res.status(400).json({ errors: errors.array() });\n      }\n\n      const user = await userService.createUser(req.body);\n      res.status(201).json({\n        success: true,\n        data: user,\n        message: 'User created successfully'\n      });\n    } catch (error) {\n      next(error);\n    }\n  }\n\n  async getUserById(req, res, next) {\n    try {\n      const { id } = req.params;\n      const user = await userService.getUserById(id);\n      \n      if (!user) {\n        return res.status(404).json({\n          success: false,\n          message: 'User not found'\n        });\n      }\n\n      res.json({\n        success: true,\n        data: user\n      });\n    } catch (error) {\n      next(error);\n    }\n  }\n}\n\nmodule.exports = new UserController();\n</code></pre>\n<h2>Docker Configuration</h2>\n<h3>Dockerfile</h3>\n<pre><code class=\"language-dockerfile\"># Multi-stage build for production optimization\nFROM node:18-alpine AS builder\n\nWORKDIR /app\nCOPY package*.json ./\nRUN npm ci --only=production &#x26;&#x26; npm cache clean --force\n\nFROM node:18-alpine AS runtime\n\n# Create non-root user for security\nRUN addgroup -g 1001 -S nodejs\nRUN adduser -S nodeuser -u 1001\n\nWORKDIR /app\n\n# Copy dependencies from builder stage\nCOPY --from=builder /app/node_modules ./node_modules\nCOPY --chown=nodeuser:nodejs . .\n\n# Switch to non-root user\nUSER nodeuser\n\n# Expose port\nEXPOSE 3000\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \\\n  CMD node healthcheck.js\n\n# Start the application\nCMD [\"node\", \"src/server.js\"]\n</code></pre>\n<h3>Docker Compose for Development</h3>\n<pre><code class=\"language-yaml\"># docker-compose.yml\nversion: '3.8'\n\nservices:\n  user-service:\n    build: .\n    ports:\n      - \"3001:3000\"\n    environment:\n      - NODE_ENV=development\n      - DATABASE_URL=postgresql://user:password@postgres:5432/userdb\n      - REDIS_URL=redis://redis:6379\n    depends_on:\n      - postgres\n      - redis\n    volumes:\n      - .:/app\n      - /app/node_modules\n    command: npm run dev\n\n  order-service:\n    build: ../order-service\n    ports:\n      - \"3002:3000\"\n    environment:\n      - NODE_ENV=development\n      - DATABASE_URL=postgresql://user:password@postgres:5432/orderdb\n      - USER_SERVICE_URL=http://user-service:3000\n    depends_on:\n      - postgres\n      - user-service\n\n  postgres:\n    image: postgres:15-alpine\n    environment:\n      - POSTGRES_USER=user\n      - POSTGRES_PASSWORD=password\n      - POSTGRES_DB=microservices\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n\n  redis:\n    image: redis:7-alpine\n    ports:\n      - \"6379:6379\"\n    volumes:\n      - redis_data:/data\n\n  nginx:\n    image: nginx:alpine\n    ports:\n      - \"80:80\"\n    volumes:\n      - ./nginx.conf:/etc/nginx/nginx.conf\n    depends_on:\n      - user-service\n      - order-service\n\nvolumes:\n  postgres_data:\n  redis_data:\n</code></pre>\n<h2>Scaling Strategies</h2>\n<h3>Horizontal Scaling</h3>\n<pre><code class=\"language-yaml\"># docker-compose.prod.yml\nversion: '3.8'\n\nservices:\n  user-service:\n    build: .\n    deploy:\n      replicas: 3\n      resources:\n        limits:\n          cpus: '0.5'\n          memory: 512M\n        reservations:\n          cpus: '0.25'\n          memory: 256M\n      restart_policy:\n        condition: on-failure\n        delay: 5s\n        max_attempts: 3\n    environment:\n      - NODE_ENV=production\n      - DATABASE_URL=${DATABASE_URL}\n      - REDIS_URL=${REDIS_URL}\n</code></pre>\n<h3>Load Balancing with Nginx</h3>\n<pre><code class=\"language-nginx\"># nginx.conf\nupstream user_service {\n    least_conn;\n    server user-service-1:3000;\n    server user-service-2:3000;\n    server user-service-3:3000;\n}\n\nupstream order_service {\n    least_conn;\n    server order-service-1:3000;\n    server order-service-2:3000;\n}\n\nserver {\n    listen 80;\n    \n    location /api/v1/users {\n        proxy_pass http://user_service;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        \n        # Health check\n        proxy_next_upstream error timeout invalid_header http_500 http_502 http_503;\n        proxy_connect_timeout 5s;\n        proxy_send_timeout 10s;\n        proxy_read_timeout 10s;\n    }\n    \n    location /api/v1/orders {\n        proxy_pass http://order_service;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n    }\n}\n</code></pre>\n<h2>Monitoring and Observability</h2>\n<h3>Application Metrics</h3>\n<pre><code class=\"language-javascript\">// src/middleware/metrics.js\nconst prometheus = require('prom-client');\n\n// Create metrics\nconst httpRequestDuration = new prometheus.Histogram({\n  name: 'http_request_duration_seconds',\n  help: 'Duration of HTTP requests in seconds',\n  labelNames: ['method', 'route', 'status_code']\n});\n\nconst httpRequestTotal = new prometheus.Counter({\n  name: 'http_requests_total',\n  help: 'Total number of HTTP requests',\n  labelNames: ['method', 'route', 'status_code']\n});\n\nconst activeConnections = new prometheus.Gauge({\n  name: 'active_connections',\n  help: 'Number of active connections'\n});\n\n// Middleware to collect metrics\nconst metricsMiddleware = (req, res, next) => {\n  const start = Date.now();\n  \n  res.on('finish', () => {\n    const duration = (Date.now() - start) / 1000;\n    const route = req.route ? req.route.path : req.path;\n    \n    httpRequestDuration\n      .labels(req.method, route, res.statusCode)\n      .observe(duration);\n    \n    httpRequestTotal\n      .labels(req.method, route, res.statusCode)\n      .inc();\n  });\n  \n  next();\n};\n\nmodule.exports = { metricsMiddleware, prometheus };\n</code></pre>\n<h3>Structured Logging</h3>\n<pre><code class=\"language-javascript\">// src/utils/logger.js\nconst winston = require('winston');\n\nconst logger = winston.createLogger({\n  level: process.env.LOG_LEVEL || 'info',\n  format: winston.format.combine(\n    winston.format.timestamp(),\n    winston.format.errors({ stack: true }),\n    winston.format.json()\n  ),\n  defaultMeta: {\n    service: process.env.SERVICE_NAME || 'microservice',\n    version: process.env.SERVICE_VERSION || '1.0.0'\n  },\n  transports: [\n    new winston.transports.File({ filename: 'logs/error.log', level: 'error' }),\n    new winston.transports.File({ filename: 'logs/combined.log' }),\n    new winston.transports.Console({\n      format: winston.format.combine(\n        winston.format.colorize(),\n        winston.format.simple()\n      )\n    })\n  ]\n});\n\nmodule.exports = logger;\n</code></pre>\n<h2>Best Practices</h2>\n<h3>1. Circuit Breaker Pattern</h3>\n<pre><code class=\"language-javascript\">// src/utils/circuitBreaker.js\nclass CircuitBreaker {\n  constructor(request, options = {}) {\n    this.request = request;\n    this.state = 'CLOSED';\n    this.failureCount = 0;\n    this.successCount = 0;\n    this.nextAttempt = Date.now();\n    \n    this.failureThreshold = options.failureThreshold || 5;\n    this.successThreshold = options.successThreshold || 2;\n    this.timeout = options.timeout || 60000;\n  }\n\n  async call(...args) {\n    if (this.state === 'OPEN') {\n      if (this.nextAttempt &#x3C;= Date.now()) {\n        this.state = 'HALF_OPEN';\n      } else {\n        throw new Error('Circuit breaker is OPEN');\n      }\n    }\n\n    try {\n      const result = await this.request(...args);\n      return this.onSuccess(result);\n    } catch (error) {\n      return this.onFailure(error);\n    }\n  }\n\n  onSuccess(result) {\n    this.failureCount = 0;\n    \n    if (this.state === 'HALF_OPEN') {\n      this.successCount++;\n      if (this.successCount >= this.successThreshold) {\n        this.state = 'CLOSED';\n        this.successCount = 0;\n      }\n    }\n    \n    return result;\n  }\n\n  onFailure(error) {\n    this.failureCount++;\n    \n    if (this.failureCount >= this.failureThreshold) {\n      this.state = 'OPEN';\n      this.nextAttempt = Date.now() + this.timeout;\n    }\n    \n    throw error;\n  }\n}\n\nmodule.exports = CircuitBreaker;\n</code></pre>\n<h3>2. Graceful Shutdown</h3>\n<pre><code class=\"language-javascript\">// src/server.js\nconst app = require('./app');\nconst logger = require('./utils/logger');\n\nconst PORT = process.env.PORT || 3000;\nconst server = app.listen(PORT, () => {\n  logger.info(`Server running on port ${PORT}`);\n});\n\n// Graceful shutdown\nprocess.on('SIGTERM', gracefulShutdown);\nprocess.on('SIGINT', gracefulShutdown);\n\nfunction gracefulShutdown(signal) {\n  logger.info(`Received ${signal}. Starting graceful shutdown...`);\n  \n  server.close((err) => {\n    if (err) {\n      logger.error('Error during server shutdown:', err);\n      process.exit(1);\n    }\n    \n    logger.info('Server closed successfully');\n    process.exit(0);\n  });\n  \n  // Force shutdown after 30 seconds\n  setTimeout(() => {\n    logger.error('Forced shutdown after timeout');\n    process.exit(1);\n  }, 30000);\n}\n</code></pre>\n<h2>Deployment to Production</h2>\n<h3>Kubernetes Deployment</h3>\n<pre><code class=\"language-yaml\"># k8s/user-service-deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: user-service\n  labels:\n    app: user-service\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: user-service\n  template:\n    metadata:\n      labels:\n        app: user-service\n    spec:\n      containers:\n      - name: user-service\n        image: your-registry/user-service:latest\n        ports:\n        - containerPort: 3000\n        env:\n        - name: NODE_ENV\n          value: \"production\"\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: database-secret\n              key: url\n        resources:\n          requests:\n            memory: \"256Mi\"\n            cpu: \"250m\"\n          limits:\n            memory: \"512Mi\"\n            cpu: \"500m\"\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 3000\n          initialDelaySeconds: 30\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 3000\n          initialDelaySeconds: 5\n          periodSeconds: 5\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: user-service\nspec:\n  selector:\n    app: user-service\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 3000\n  type: ClusterIP\n</code></pre>\n<h2>Conclusion</h2>\n<p>Building scalable microservices with Node.js and Docker requires careful attention to architecture, implementation, and operational concerns. Key takeaways:</p>\n<ol>\n<li><strong>Design for failure</strong>: Implement circuit breakers, retries, and graceful degradation</li>\n<li><strong>Monitor everything</strong>: Use structured logging, metrics, and distributed tracing</li>\n<li><strong>Automate deployment</strong>: Use CI/CD pipelines and infrastructure as code</li>\n<li><strong>Plan for scale</strong>: Design services to be stateless and horizontally scalable</li>\n<li><strong>Security first</strong>: Implement authentication, authorization, and secure communication</li>\n</ol>\n<p>By following these patterns and practices, you can build microservices that scale to handle millions of requests while maintaining reliability and performance.</p>\n<p>The complete source code for this example is available on <a href=\"https://github.com/aayushpathak/scalable-microservices-example\">GitHub</a>.</p>\n",
    "author": "Aayush Pathak",
    "date": "2024-01-15",
    "readTime": 12,
    "category": "Backend",
    "tags": [
      "Node.js",
      "Docker",
      "Microservices",
      "Architecture"
    ],
    "image": "/blog/images/microservices-architecture.jpg",
    "featured": true
  },
  {
    "slug": "2024-01-10-database-optimization",
    "title": "Database Optimization: From Slow Queries to Lightning Fast",
    "excerpt": "Learn advanced database optimization techniques including indexing strategies, query optimization, and connection pooling for high-performance applications.",
    "content": "\n# Database Optimization: From Slow Queries to Lightning Fast\n\n## Introduction\n\nDatabase performance is often the bottleneck in web applications. A poorly optimized database can turn a lightning-fast application into a sluggish nightmare. In this comprehensive guide, we'll explore proven techniques to optimize database performance, focusing on PostgreSQL but with principles applicable to most relational databases.\n\n## Understanding Database Performance\n\n### The Performance Stack\n\nDatabase performance depends on several layers:\n\n1. **Hardware**: CPU, RAM, storage (SSD vs HDD)\n2. **Operating System**: File system, memory management\n3. **Database Engine**: Query planner, buffer management\n4. **Schema Design**: Tables, indexes, relationships\n5. **Query Design**: SQL structure, joins, subqueries\n6. **Application Layer**: Connection pooling, caching\n\n### Key Performance Metrics\n\n```sql\n-- Monitor key performance indicators\nSELECT \n    schemaname,\n    tablename,\n    attname,\n    n_distinct,\n    correlation\nFROM pg_stats \nWHERE schemaname = 'public'\nORDER BY n_distinct DESC;\n\n-- Check slow queries\nSELECT \n    query,\n    calls,\n    total_time,\n    mean_time,\n    rows\nFROM pg_stat_statements \nORDER BY mean_time DESC \nLIMIT 10;\n```\n\n## Indexing Strategies\n\n### Understanding Index Types\n\n#### B-Tree Indexes (Default)\n```sql\n-- Standard B-tree index for equality and range queries\nCREATE INDEX idx_users_email ON users(email);\nCREATE INDEX idx_orders_created_at ON orders(created_at);\n\n-- Composite indexes for multi-column queries\nCREATE INDEX idx_orders_user_status ON orders(user_id, status);\n```\n\n#### Hash Indexes\n```sql\n-- Hash indexes for equality comparisons only\nCREATE INDEX idx_users_id_hash ON users USING HASH(id);\n```\n\n#### GIN Indexes for JSON and Arrays\n```sql\n-- GIN index for JSONB columns\nCREATE INDEX idx_users_metadata ON users USING GIN(metadata);\n\n-- GIN index for array columns\nCREATE INDEX idx_posts_tags ON posts USING GIN(tags);\n\n-- Example queries that benefit from GIN indexes\nSELECT * FROM users WHERE metadata @> '{\"premium\": true}';\nSELECT * FROM posts WHERE tags && ARRAY['postgresql', 'optimization'];\n```\n\n#### Partial Indexes\n```sql\n-- Index only active users\nCREATE INDEX idx_users_active_email ON users(email) \nWHERE status = 'active';\n\n-- Index only recent orders\nCREATE INDEX idx_orders_recent ON orders(created_at) \nWHERE created_at > '2024-01-01';\n```\n\n### Index Optimization Techniques\n\n#### Covering Indexes\n```sql\n-- Include additional columns to avoid table lookups\nCREATE INDEX idx_orders_covering ON orders(user_id) \nINCLUDE (total_amount, created_at, status);\n\n-- Query can be satisfied entirely from the index\nSELECT total_amount, created_at, status \nFROM orders \nWHERE user_id = 123;\n```\n\n#### Expression Indexes\n```sql\n-- Index on computed values\nCREATE INDEX idx_users_lower_email ON users(LOWER(email));\nCREATE INDEX idx_orders_year ON orders(EXTRACT(YEAR FROM created_at));\n\n-- Queries using the same expression will use the index\nSELECT * FROM users WHERE LOWER(email) = 'user@example.com';\nSELECT * FROM orders WHERE EXTRACT(YEAR FROM created_at) = 2024;\n```\n\n## Query Optimization\n\n### Analyzing Query Performance\n\n```sql\n-- Use EXPLAIN ANALYZE to understand query execution\nEXPLAIN (ANALYZE, BUFFERS, FORMAT JSON) \nSELECT u.name, COUNT(o.id) as order_count\nFROM users u\nLEFT JOIN orders o ON u.id = o.user_id\nWHERE u.created_at > '2024-01-01'\nGROUP BY u.id, u.name\nORDER BY order_count DESC\nLIMIT 10;\n```\n\n### Common Query Anti-Patterns\n\n#### Avoid SELECT *\n```sql\n-- Bad: Retrieves unnecessary data\nSELECT * FROM users WHERE email = 'user@example.com';\n\n-- Good: Select only needed columns\nSELECT id, name, email FROM users WHERE email = 'user@example.com';\n```\n\n#### Optimize WHERE Clauses\n```sql\n-- Bad: Function on column prevents index usage\nSELECT * FROM orders WHERE YEAR(created_at) = 2024;\n\n-- Good: Use range conditions\nSELECT * FROM orders \nWHERE created_at >= '2024-01-01' \nAND created_at < '2025-01-01';\n```\n\n#### Efficient JOINs\n```sql\n-- Bad: Cartesian product then filter\nSELECT u.name, o.total_amount\nFROM users u, orders o\nWHERE u.id = o.user_id AND u.status = 'active';\n\n-- Good: Explicit JOIN with proper conditions\nSELECT u.name, o.total_amount\nFROM users u\nINNER JOIN orders o ON u.id = o.user_id\nWHERE u.status = 'active';\n```\n\n### Advanced Query Techniques\n\n#### Window Functions vs Subqueries\n```sql\n-- Subquery approach (less efficient)\nSELECT \n    user_id,\n    total_amount,\n    (SELECT AVG(total_amount) FROM orders o2 WHERE o2.user_id = o1.user_id) as avg_amount\nFROM orders o1;\n\n-- Window function approach (more efficient)\nSELECT \n    user_id,\n    total_amount,\n    AVG(total_amount) OVER (PARTITION BY user_id) as avg_amount\nFROM orders;\n```\n\n#### Common Table Expressions (CTEs)\n```sql\n-- Recursive CTE for hierarchical data\nWITH RECURSIVE category_tree AS (\n    -- Base case: root categories\n    SELECT id, name, parent_id, 0 as level\n    FROM categories \n    WHERE parent_id IS NULL\n    \n    UNION ALL\n    \n    -- Recursive case: child categories\n    SELECT c.id, c.name, c.parent_id, ct.level + 1\n    FROM categories c\n    INNER JOIN category_tree ct ON c.parent_id = ct.id\n)\nSELECT * FROM category_tree ORDER BY level, name;\n```\n\n## Connection Pooling\n\n### Why Connection Pooling Matters\n\nDatabase connections are expensive resources. Creating a new connection involves:\n- TCP handshake\n- Authentication\n- Session initialization\n- Memory allocation\n\n### Implementing Connection Pooling with Node.js\n\n```javascript\n// Using pg-pool for PostgreSQL\nconst { Pool } = require('pg');\n\nconst pool = new Pool({\n  user: process.env.DB_USER,\n  host: process.env.DB_HOST,\n  database: process.env.DB_NAME,\n  password: process.env.DB_PASSWORD,\n  port: process.env.DB_PORT,\n  \n  // Pool configuration\n  max: 20,                    // Maximum number of connections\n  min: 5,                     // Minimum number of connections\n  idleTimeoutMillis: 30000,   // Close idle connections after 30s\n  connectionTimeoutMillis: 2000, // Timeout when acquiring connection\n  maxUses: 7500,              // Close connection after 7500 uses\n});\n\n// Connection pool monitoring\npool.on('connect', (client) => {\n  console.log('New client connected');\n});\n\npool.on('error', (err, client) => {\n  console.error('Unexpected error on idle client', err);\n});\n\n// Usage example\nasync function getUserById(id) {\n  const client = await pool.connect();\n  try {\n    const result = await client.query('SELECT * FROM users WHERE id = $1', [id]);\n    return result.rows[0];\n  } finally {\n    client.release(); // Return connection to pool\n  }\n}\n\n// Graceful shutdown\nprocess.on('SIGINT', async () => {\n  await pool.end();\n  console.log('Connection pool closed');\n  process.exit(0);\n});\n```\n\n### Advanced Pool Configuration\n\n```javascript\n// Custom pool with retry logic and monitoring\nclass DatabasePool {\n  constructor(config) {\n    this.pool = new Pool({\n      ...config,\n      max: 20,\n      min: 5,\n      idleTimeoutMillis: 30000,\n      connectionTimeoutMillis: 2000,\n    });\n    \n    this.setupMonitoring();\n  }\n  \n  setupMonitoring() {\n    setInterval(() => {\n      console.log('Pool stats:', {\n        totalCount: this.pool.totalCount,\n        idleCount: this.pool.idleCount,\n        waitingCount: this.pool.waitingCount\n      });\n    }, 60000); // Log every minute\n  }\n  \n  async query(text, params, retries = 3) {\n    for (let attempt = 1; attempt <= retries; attempt++) {\n      try {\n        const client = await this.pool.connect();\n        try {\n          const start = Date.now();\n          const result = await client.query(text, params);\n          const duration = Date.now() - start;\n          \n          // Log slow queries\n          if (duration > 1000) {\n            console.warn(`Slow query (${duration}ms):`, text);\n          }\n          \n          return result;\n        } finally {\n          client.release();\n        }\n      } catch (error) {\n        console.error(`Query attempt ${attempt} failed:`, error.message);\n        \n        if (attempt === retries) {\n          throw error;\n        }\n        \n        // Wait before retry\n        await new Promise(resolve => setTimeout(resolve, 1000 * attempt));\n      }\n    }\n  }\n  \n  async transaction(callback) {\n    const client = await this.pool.connect();\n    try {\n      await client.query('BEGIN');\n      const result = await callback(client);\n      await client.query('COMMIT');\n      return result;\n    } catch (error) {\n      await client.query('ROLLBACK');\n      throw error;\n    } finally {\n      client.release();\n    }\n  }\n}\n```\n\n## Database Schema Optimization\n\n### Normalization vs Denormalization\n\n#### Proper Normalization\n```sql\n-- Normalized schema (3NF)\nCREATE TABLE users (\n    id SERIAL PRIMARY KEY,\n    email VARCHAR(255) UNIQUE NOT NULL,\n    name VARCHAR(255) NOT NULL,\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\nCREATE TABLE user_profiles (\n    user_id INTEGER PRIMARY KEY REFERENCES users(id),\n    bio TEXT,\n    avatar_url VARCHAR(500),\n    location VARCHAR(255)\n);\n\nCREATE TABLE orders (\n    id SERIAL PRIMARY KEY,\n    user_id INTEGER NOT NULL REFERENCES users(id),\n    total_amount DECIMAL(10,2) NOT NULL,\n    status VARCHAR(50) NOT NULL,\n    created_at TIMESTAMP DEFAULT NOW()\n);\n```\n\n#### Strategic Denormalization\n```sql\n-- Denormalized for read performance\nCREATE TABLE order_summary (\n    id SERIAL PRIMARY KEY,\n    user_id INTEGER NOT NULL,\n    user_name VARCHAR(255) NOT NULL,  -- Denormalized\n    user_email VARCHAR(255) NOT NULL, -- Denormalized\n    total_amount DECIMAL(10,2) NOT NULL,\n    status VARCHAR(50) NOT NULL,\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\n-- Maintain consistency with triggers\nCREATE OR REPLACE FUNCTION update_order_summary()\nRETURNS TRIGGER AS $$\nBEGIN\n    IF TG_OP = 'UPDATE' THEN\n        UPDATE order_summary \n        SET user_name = NEW.name, user_email = NEW.email\n        WHERE user_id = NEW.id;\n    END IF;\n    RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\nCREATE TRIGGER user_update_trigger\n    AFTER UPDATE ON users\n    FOR EACH ROW\n    EXECUTE FUNCTION update_order_summary();\n```\n\n### Data Types Optimization\n\n```sql\n-- Choose appropriate data types\nCREATE TABLE optimized_table (\n    id BIGSERIAL PRIMARY KEY,           -- Use BIGSERIAL for high-volume tables\n    status SMALLINT NOT NULL,           -- SMALLINT instead of INTEGER for small ranges\n    price DECIMAL(10,2) NOT NULL,       -- DECIMAL for exact monetary values\n    created_at TIMESTAMPTZ DEFAULT NOW(), -- TIMESTAMPTZ for timezone awareness\n    metadata JSONB,                     -- JSONB instead of JSON for better performance\n    tags TEXT[],                        -- Array type for multiple values\n    is_active BOOLEAN DEFAULT true      -- BOOLEAN for true/false values\n);\n\n-- Add constraints for data integrity and query optimization\nALTER TABLE optimized_table \nADD CONSTRAINT check_status CHECK (status IN (1, 2, 3, 4, 5));\n\nALTER TABLE optimized_table \nADD CONSTRAINT check_price CHECK (price >= 0);\n```\n\n## Caching Strategies\n\n### Application-Level Caching\n\n```javascript\n// Redis caching layer\nconst redis = require('redis');\nconst client = redis.createClient({\n  host: process.env.REDIS_HOST,\n  port: process.env.REDIS_PORT,\n  password: process.env.REDIS_PASSWORD,\n});\n\nclass CachedUserService {\n  constructor(db, cache) {\n    this.db = db;\n    this.cache = cache;\n  }\n  \n  async getUserById(id) {\n    const cacheKey = `user:${id}`;\n    \n    // Try cache first\n    const cached = await this.cache.get(cacheKey);\n    if (cached) {\n      return JSON.parse(cached);\n    }\n    \n    // Fallback to database\n    const user = await this.db.query(\n      'SELECT * FROM users WHERE id = $1', \n      [id]\n    );\n    \n    if (user.rows[0]) {\n      // Cache for 1 hour\n      await this.cache.setex(cacheKey, 3600, JSON.stringify(user.rows[0]));\n      return user.rows[0];\n    }\n    \n    return null;\n  }\n  \n  async updateUser(id, updates) {\n    // Update database\n    const result = await this.db.query(\n      'UPDATE users SET name = $1, email = $2 WHERE id = $3 RETURNING *',\n      [updates.name, updates.email, id]\n    );\n    \n    // Invalidate cache\n    await this.cache.del(`user:${id}`);\n    \n    return result.rows[0];\n  }\n}\n```\n\n### Query Result Caching\n\n```javascript\n// Intelligent query caching\nclass QueryCache {\n  constructor(cache, ttl = 300) {\n    this.cache = cache;\n    this.ttl = ttl;\n  }\n  \n  generateKey(query, params) {\n    const crypto = require('crypto');\n    const hash = crypto.createHash('md5');\n    hash.update(query + JSON.stringify(params));\n    return `query:${hash.digest('hex')}`;\n  }\n  \n  async execute(db, query, params, cacheTTL = this.ttl) {\n    const cacheKey = this.generateKey(query, params);\n    \n    // Check cache\n    const cached = await this.cache.get(cacheKey);\n    if (cached) {\n      return JSON.parse(cached);\n    }\n    \n    // Execute query\n    const result = await db.query(query, params);\n    \n    // Cache result\n    if (result.rows.length > 0) {\n      await this.cache.setex(\n        cacheKey, \n        cacheTTL, \n        JSON.stringify(result.rows)\n      );\n    }\n    \n    return result.rows;\n  }\n}\n```\n\n## Monitoring and Maintenance\n\n### Performance Monitoring\n\n```sql\n-- Create monitoring views\nCREATE VIEW slow_queries AS\nSELECT \n    query,\n    calls,\n    total_time,\n    mean_time,\n    stddev_time,\n    rows,\n    100.0 * shared_blks_hit / nullif(shared_blks_hit + shared_blks_read, 0) AS hit_percent\nFROM pg_stat_statements \nWHERE calls > 100\nORDER BY mean_time DESC;\n\n-- Index usage statistics\nCREATE VIEW index_usage AS\nSELECT \n    schemaname,\n    tablename,\n    indexname,\n    idx_tup_read,\n    idx_tup_fetch,\n    idx_scan,\n    CASE \n        WHEN idx_scan = 0 THEN 'Never used'\n        WHEN idx_scan < 100 THEN 'Rarely used'\n        ELSE 'Frequently used'\n    END as usage_level\nFROM pg_stat_user_indexes\nORDER BY idx_scan DESC;\n```\n\n### Automated Maintenance\n\n```sql\n-- Automated VACUUM and ANALYZE\nCREATE OR REPLACE FUNCTION auto_maintenance()\nRETURNS void AS $$\nDECLARE\n    table_record RECORD;\nBEGIN\n    -- Auto-vacuum tables with high update/delete activity\n    FOR table_record IN \n        SELECT schemaname, tablename, n_tup_upd, n_tup_del\n        FROM pg_stat_user_tables \n        WHERE n_tup_upd + n_tup_del > 1000\n    LOOP\n        EXECUTE format('VACUUM ANALYZE %I.%I', \n                      table_record.schemaname, \n                      table_record.tablename);\n        \n        RAISE NOTICE 'Vacuumed table %.%', \n                     table_record.schemaname, \n                     table_record.tablename;\n    END LOOP;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Schedule maintenance (requires pg_cron extension)\nSELECT cron.schedule('auto-maintenance', '0 2 * * *', 'SELECT auto_maintenance();');\n```\n\n### Health Check Queries\n\n```javascript\n// Database health monitoring\nclass DatabaseHealth {\n  constructor(db) {\n    this.db = db;\n  }\n  \n  async checkHealth() {\n    const checks = await Promise.allSettled([\n      this.checkConnectivity(),\n      this.checkSlowQueries(),\n      this.checkIndexUsage(),\n      this.checkTableBloat(),\n      this.checkReplicationLag()\n    ]);\n    \n    return {\n      timestamp: new Date().toISOString(),\n      checks: checks.map((result, index) => ({\n        name: ['connectivity', 'slow_queries', 'index_usage', 'table_bloat', 'replication_lag'][index],\n        status: result.status,\n        data: result.value || result.reason\n      }))\n    };\n  }\n  \n  async checkConnectivity() {\n    const result = await this.db.query('SELECT NOW() as current_time');\n    return { healthy: true, response_time: Date.now() };\n  }\n  \n  async checkSlowQueries() {\n    const result = await this.db.query(`\n      SELECT COUNT(*) as slow_query_count\n      FROM pg_stat_statements \n      WHERE mean_time > 1000 AND calls > 10\n    `);\n    \n    const count = parseInt(result.rows[0].slow_query_count);\n    return {\n      healthy: count < 10,\n      slow_query_count: count\n    };\n  }\n  \n  async checkIndexUsage() {\n    const result = await this.db.query(`\n      SELECT COUNT(*) as unused_indexes\n      FROM pg_stat_user_indexes \n      WHERE idx_scan = 0\n    `);\n    \n    const count = parseInt(result.rows[0].unused_indexes);\n    return {\n      healthy: count < 5,\n      unused_indexes: count\n    };\n  }\n}\n```\n\n## Performance Testing\n\n### Load Testing with Artillery\n\n```yaml\n# artillery-config.yml\nconfig:\n  target: 'http://localhost:3000'\n  phases:\n    - duration: 60\n      arrivalRate: 10\n      name: \"Warm up\"\n    - duration: 300\n      arrivalRate: 50\n      name: \"Sustained load\"\n    - duration: 60\n      arrivalRate: 100\n      name: \"Peak load\"\n  \nscenarios:\n    - name: \"Database operations\"\n      weight: 100\n      flow:\n        - get:\n            url: \"/api/users/{{ $randomInt(1, 1000) }}\"\n        - post:\n            url: \"/api/orders\"\n            json:\n              user_id: \"{{ $randomInt(1, 1000) }}\"\n              items: [{ id: 1, quantity: 2 }]\n        - get:\n            url: \"/api/orders?user_id={{ $randomInt(1, 1000) }}\"\n```\n\n### Database Benchmarking\n\n```bash\n#!/bin/bash\n# benchmark.sh - Database performance testing\n\n# Connection parameters\nDB_HOST=\"localhost\"\nDB_NAME=\"testdb\"\nDB_USER=\"testuser\"\n\n# Test concurrent connections\necho \"Testing concurrent connections...\"\npgbench -h $DB_HOST -d $DB_NAME -U $DB_USER -c 10 -j 2 -T 60 -S\n\n# Test read/write performance\necho \"Testing read/write performance...\"\npgbench -h $DB_HOST -d $DB_NAME -U $DB_USER -c 20 -j 4 -T 120\n\n# Custom test with complex queries\necho \"Testing complex queries...\"\npgbench -h $DB_HOST -d $DB_NAME -U $DB_USER -c 5 -j 1 -T 60 -f complex_queries.sql\n```\n\n## Conclusion\n\nDatabase optimization is an ongoing process that requires:\n\n1. **Proper indexing strategy**: Create indexes that support your query patterns\n2. **Query optimization**: Write efficient SQL and avoid common anti-patterns\n3. **Connection pooling**: Manage database connections efficiently\n4. **Schema design**: Balance normalization with performance requirements\n5. **Caching**: Implement multiple layers of caching\n6. **Monitoring**: Continuously monitor performance and identify bottlenecks\n7. **Maintenance**: Regular VACUUM, ANALYZE, and index maintenance\n\nKey performance improvements you can expect:\n- **Proper indexing**: 10-100x query performance improvement\n- **Connection pooling**: 50-90% reduction in connection overhead\n- **Query optimization**: 2-10x improvement in complex queries\n- **Caching**: 90-99% reduction in database load for repeated queries\n\nRemember: measure first, optimize second. Always profile your specific workload and measure the impact of optimizations.\n\nThe complete examples and benchmarking scripts are available on [GitHub](https://github.com/aayushpathak/database-optimization-guide).",
    "contentHtml": "<h1>Database Optimization: From Slow Queries to Lightning Fast</h1>\n<h2>Introduction</h2>\n<p>Database performance is often the bottleneck in web applications. A poorly optimized database can turn a lightning-fast application into a sluggish nightmare. In this comprehensive guide, we'll explore proven techniques to optimize database performance, focusing on PostgreSQL but with principles applicable to most relational databases.</p>\n<h2>Understanding Database Performance</h2>\n<h3>The Performance Stack</h3>\n<p>Database performance depends on several layers:</p>\n<ol>\n<li><strong>Hardware</strong>: CPU, RAM, storage (SSD vs HDD)</li>\n<li><strong>Operating System</strong>: File system, memory management</li>\n<li><strong>Database Engine</strong>: Query planner, buffer management</li>\n<li><strong>Schema Design</strong>: Tables, indexes, relationships</li>\n<li><strong>Query Design</strong>: SQL structure, joins, subqueries</li>\n<li><strong>Application Layer</strong>: Connection pooling, caching</li>\n</ol>\n<h3>Key Performance Metrics</h3>\n<pre><code class=\"language-sql\">-- Monitor key performance indicators\nSELECT \n    schemaname,\n    tablename,\n    attname,\n    n_distinct,\n    correlation\nFROM pg_stats \nWHERE schemaname = 'public'\nORDER BY n_distinct DESC;\n\n-- Check slow queries\nSELECT \n    query,\n    calls,\n    total_time,\n    mean_time,\n    rows\nFROM pg_stat_statements \nORDER BY mean_time DESC \nLIMIT 10;\n</code></pre>\n<h2>Indexing Strategies</h2>\n<h3>Understanding Index Types</h3>\n<h4>B-Tree Indexes (Default)</h4>\n<pre><code class=\"language-sql\">-- Standard B-tree index for equality and range queries\nCREATE INDEX idx_users_email ON users(email);\nCREATE INDEX idx_orders_created_at ON orders(created_at);\n\n-- Composite indexes for multi-column queries\nCREATE INDEX idx_orders_user_status ON orders(user_id, status);\n</code></pre>\n<h4>Hash Indexes</h4>\n<pre><code class=\"language-sql\">-- Hash indexes for equality comparisons only\nCREATE INDEX idx_users_id_hash ON users USING HASH(id);\n</code></pre>\n<h4>GIN Indexes for JSON and Arrays</h4>\n<pre><code class=\"language-sql\">-- GIN index for JSONB columns\nCREATE INDEX idx_users_metadata ON users USING GIN(metadata);\n\n-- GIN index for array columns\nCREATE INDEX idx_posts_tags ON posts USING GIN(tags);\n\n-- Example queries that benefit from GIN indexes\nSELECT * FROM users WHERE metadata @> '{\"premium\": true}';\nSELECT * FROM posts WHERE tags &#x26;&#x26; ARRAY['postgresql', 'optimization'];\n</code></pre>\n<h4>Partial Indexes</h4>\n<pre><code class=\"language-sql\">-- Index only active users\nCREATE INDEX idx_users_active_email ON users(email) \nWHERE status = 'active';\n\n-- Index only recent orders\nCREATE INDEX idx_orders_recent ON orders(created_at) \nWHERE created_at > '2024-01-01';\n</code></pre>\n<h3>Index Optimization Techniques</h3>\n<h4>Covering Indexes</h4>\n<pre><code class=\"language-sql\">-- Include additional columns to avoid table lookups\nCREATE INDEX idx_orders_covering ON orders(user_id) \nINCLUDE (total_amount, created_at, status);\n\n-- Query can be satisfied entirely from the index\nSELECT total_amount, created_at, status \nFROM orders \nWHERE user_id = 123;\n</code></pre>\n<h4>Expression Indexes</h4>\n<pre><code class=\"language-sql\">-- Index on computed values\nCREATE INDEX idx_users_lower_email ON users(LOWER(email));\nCREATE INDEX idx_orders_year ON orders(EXTRACT(YEAR FROM created_at));\n\n-- Queries using the same expression will use the index\nSELECT * FROM users WHERE LOWER(email) = 'user@example.com';\nSELECT * FROM orders WHERE EXTRACT(YEAR FROM created_at) = 2024;\n</code></pre>\n<h2>Query Optimization</h2>\n<h3>Analyzing Query Performance</h3>\n<pre><code class=\"language-sql\">-- Use EXPLAIN ANALYZE to understand query execution\nEXPLAIN (ANALYZE, BUFFERS, FORMAT JSON) \nSELECT u.name, COUNT(o.id) as order_count\nFROM users u\nLEFT JOIN orders o ON u.id = o.user_id\nWHERE u.created_at > '2024-01-01'\nGROUP BY u.id, u.name\nORDER BY order_count DESC\nLIMIT 10;\n</code></pre>\n<h3>Common Query Anti-Patterns</h3>\n<h4>Avoid SELECT *</h4>\n<pre><code class=\"language-sql\">-- Bad: Retrieves unnecessary data\nSELECT * FROM users WHERE email = 'user@example.com';\n\n-- Good: Select only needed columns\nSELECT id, name, email FROM users WHERE email = 'user@example.com';\n</code></pre>\n<h4>Optimize WHERE Clauses</h4>\n<pre><code class=\"language-sql\">-- Bad: Function on column prevents index usage\nSELECT * FROM orders WHERE YEAR(created_at) = 2024;\n\n-- Good: Use range conditions\nSELECT * FROM orders \nWHERE created_at >= '2024-01-01' \nAND created_at &#x3C; '2025-01-01';\n</code></pre>\n<h4>Efficient JOINs</h4>\n<pre><code class=\"language-sql\">-- Bad: Cartesian product then filter\nSELECT u.name, o.total_amount\nFROM users u, orders o\nWHERE u.id = o.user_id AND u.status = 'active';\n\n-- Good: Explicit JOIN with proper conditions\nSELECT u.name, o.total_amount\nFROM users u\nINNER JOIN orders o ON u.id = o.user_id\nWHERE u.status = 'active';\n</code></pre>\n<h3>Advanced Query Techniques</h3>\n<h4>Window Functions vs Subqueries</h4>\n<pre><code class=\"language-sql\">-- Subquery approach (less efficient)\nSELECT \n    user_id,\n    total_amount,\n    (SELECT AVG(total_amount) FROM orders o2 WHERE o2.user_id = o1.user_id) as avg_amount\nFROM orders o1;\n\n-- Window function approach (more efficient)\nSELECT \n    user_id,\n    total_amount,\n    AVG(total_amount) OVER (PARTITION BY user_id) as avg_amount\nFROM orders;\n</code></pre>\n<h4>Common Table Expressions (CTEs)</h4>\n<pre><code class=\"language-sql\">-- Recursive CTE for hierarchical data\nWITH RECURSIVE category_tree AS (\n    -- Base case: root categories\n    SELECT id, name, parent_id, 0 as level\n    FROM categories \n    WHERE parent_id IS NULL\n    \n    UNION ALL\n    \n    -- Recursive case: child categories\n    SELECT c.id, c.name, c.parent_id, ct.level + 1\n    FROM categories c\n    INNER JOIN category_tree ct ON c.parent_id = ct.id\n)\nSELECT * FROM category_tree ORDER BY level, name;\n</code></pre>\n<h2>Connection Pooling</h2>\n<h3>Why Connection Pooling Matters</h3>\n<p>Database connections are expensive resources. Creating a new connection involves:</p>\n<ul>\n<li>TCP handshake</li>\n<li>Authentication</li>\n<li>Session initialization</li>\n<li>Memory allocation</li>\n</ul>\n<h3>Implementing Connection Pooling with Node.js</h3>\n<pre><code class=\"language-javascript\">// Using pg-pool for PostgreSQL\nconst { Pool } = require('pg');\n\nconst pool = new Pool({\n  user: process.env.DB_USER,\n  host: process.env.DB_HOST,\n  database: process.env.DB_NAME,\n  password: process.env.DB_PASSWORD,\n  port: process.env.DB_PORT,\n  \n  // Pool configuration\n  max: 20,                    // Maximum number of connections\n  min: 5,                     // Minimum number of connections\n  idleTimeoutMillis: 30000,   // Close idle connections after 30s\n  connectionTimeoutMillis: 2000, // Timeout when acquiring connection\n  maxUses: 7500,              // Close connection after 7500 uses\n});\n\n// Connection pool monitoring\npool.on('connect', (client) => {\n  console.log('New client connected');\n});\n\npool.on('error', (err, client) => {\n  console.error('Unexpected error on idle client', err);\n});\n\n// Usage example\nasync function getUserById(id) {\n  const client = await pool.connect();\n  try {\n    const result = await client.query('SELECT * FROM users WHERE id = $1', [id]);\n    return result.rows[0];\n  } finally {\n    client.release(); // Return connection to pool\n  }\n}\n\n// Graceful shutdown\nprocess.on('SIGINT', async () => {\n  await pool.end();\n  console.log('Connection pool closed');\n  process.exit(0);\n});\n</code></pre>\n<h3>Advanced Pool Configuration</h3>\n<pre><code class=\"language-javascript\">// Custom pool with retry logic and monitoring\nclass DatabasePool {\n  constructor(config) {\n    this.pool = new Pool({\n      ...config,\n      max: 20,\n      min: 5,\n      idleTimeoutMillis: 30000,\n      connectionTimeoutMillis: 2000,\n    });\n    \n    this.setupMonitoring();\n  }\n  \n  setupMonitoring() {\n    setInterval(() => {\n      console.log('Pool stats:', {\n        totalCount: this.pool.totalCount,\n        idleCount: this.pool.idleCount,\n        waitingCount: this.pool.waitingCount\n      });\n    }, 60000); // Log every minute\n  }\n  \n  async query(text, params, retries = 3) {\n    for (let attempt = 1; attempt &#x3C;= retries; attempt++) {\n      try {\n        const client = await this.pool.connect();\n        try {\n          const start = Date.now();\n          const result = await client.query(text, params);\n          const duration = Date.now() - start;\n          \n          // Log slow queries\n          if (duration > 1000) {\n            console.warn(`Slow query (${duration}ms):`, text);\n          }\n          \n          return result;\n        } finally {\n          client.release();\n        }\n      } catch (error) {\n        console.error(`Query attempt ${attempt} failed:`, error.message);\n        \n        if (attempt === retries) {\n          throw error;\n        }\n        \n        // Wait before retry\n        await new Promise(resolve => setTimeout(resolve, 1000 * attempt));\n      }\n    }\n  }\n  \n  async transaction(callback) {\n    const client = await this.pool.connect();\n    try {\n      await client.query('BEGIN');\n      const result = await callback(client);\n      await client.query('COMMIT');\n      return result;\n    } catch (error) {\n      await client.query('ROLLBACK');\n      throw error;\n    } finally {\n      client.release();\n    }\n  }\n}\n</code></pre>\n<h2>Database Schema Optimization</h2>\n<h3>Normalization vs Denormalization</h3>\n<h4>Proper Normalization</h4>\n<pre><code class=\"language-sql\">-- Normalized schema (3NF)\nCREATE TABLE users (\n    id SERIAL PRIMARY KEY,\n    email VARCHAR(255) UNIQUE NOT NULL,\n    name VARCHAR(255) NOT NULL,\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\nCREATE TABLE user_profiles (\n    user_id INTEGER PRIMARY KEY REFERENCES users(id),\n    bio TEXT,\n    avatar_url VARCHAR(500),\n    location VARCHAR(255)\n);\n\nCREATE TABLE orders (\n    id SERIAL PRIMARY KEY,\n    user_id INTEGER NOT NULL REFERENCES users(id),\n    total_amount DECIMAL(10,2) NOT NULL,\n    status VARCHAR(50) NOT NULL,\n    created_at TIMESTAMP DEFAULT NOW()\n);\n</code></pre>\n<h4>Strategic Denormalization</h4>\n<pre><code class=\"language-sql\">-- Denormalized for read performance\nCREATE TABLE order_summary (\n    id SERIAL PRIMARY KEY,\n    user_id INTEGER NOT NULL,\n    user_name VARCHAR(255) NOT NULL,  -- Denormalized\n    user_email VARCHAR(255) NOT NULL, -- Denormalized\n    total_amount DECIMAL(10,2) NOT NULL,\n    status VARCHAR(50) NOT NULL,\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\n-- Maintain consistency with triggers\nCREATE OR REPLACE FUNCTION update_order_summary()\nRETURNS TRIGGER AS $$\nBEGIN\n    IF TG_OP = 'UPDATE' THEN\n        UPDATE order_summary \n        SET user_name = NEW.name, user_email = NEW.email\n        WHERE user_id = NEW.id;\n    END IF;\n    RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\nCREATE TRIGGER user_update_trigger\n    AFTER UPDATE ON users\n    FOR EACH ROW\n    EXECUTE FUNCTION update_order_summary();\n</code></pre>\n<h3>Data Types Optimization</h3>\n<pre><code class=\"language-sql\">-- Choose appropriate data types\nCREATE TABLE optimized_table (\n    id BIGSERIAL PRIMARY KEY,           -- Use BIGSERIAL for high-volume tables\n    status SMALLINT NOT NULL,           -- SMALLINT instead of INTEGER for small ranges\n    price DECIMAL(10,2) NOT NULL,       -- DECIMAL for exact monetary values\n    created_at TIMESTAMPTZ DEFAULT NOW(), -- TIMESTAMPTZ for timezone awareness\n    metadata JSONB,                     -- JSONB instead of JSON for better performance\n    tags TEXT[],                        -- Array type for multiple values\n    is_active BOOLEAN DEFAULT true      -- BOOLEAN for true/false values\n);\n\n-- Add constraints for data integrity and query optimization\nALTER TABLE optimized_table \nADD CONSTRAINT check_status CHECK (status IN (1, 2, 3, 4, 5));\n\nALTER TABLE optimized_table \nADD CONSTRAINT check_price CHECK (price >= 0);\n</code></pre>\n<h2>Caching Strategies</h2>\n<h3>Application-Level Caching</h3>\n<pre><code class=\"language-javascript\">// Redis caching layer\nconst redis = require('redis');\nconst client = redis.createClient({\n  host: process.env.REDIS_HOST,\n  port: process.env.REDIS_PORT,\n  password: process.env.REDIS_PASSWORD,\n});\n\nclass CachedUserService {\n  constructor(db, cache) {\n    this.db = db;\n    this.cache = cache;\n  }\n  \n  async getUserById(id) {\n    const cacheKey = `user:${id}`;\n    \n    // Try cache first\n    const cached = await this.cache.get(cacheKey);\n    if (cached) {\n      return JSON.parse(cached);\n    }\n    \n    // Fallback to database\n    const user = await this.db.query(\n      'SELECT * FROM users WHERE id = $1', \n      [id]\n    );\n    \n    if (user.rows[0]) {\n      // Cache for 1 hour\n      await this.cache.setex(cacheKey, 3600, JSON.stringify(user.rows[0]));\n      return user.rows[0];\n    }\n    \n    return null;\n  }\n  \n  async updateUser(id, updates) {\n    // Update database\n    const result = await this.db.query(\n      'UPDATE users SET name = $1, email = $2 WHERE id = $3 RETURNING *',\n      [updates.name, updates.email, id]\n    );\n    \n    // Invalidate cache\n    await this.cache.del(`user:${id}`);\n    \n    return result.rows[0];\n  }\n}\n</code></pre>\n<h3>Query Result Caching</h3>\n<pre><code class=\"language-javascript\">// Intelligent query caching\nclass QueryCache {\n  constructor(cache, ttl = 300) {\n    this.cache = cache;\n    this.ttl = ttl;\n  }\n  \n  generateKey(query, params) {\n    const crypto = require('crypto');\n    const hash = crypto.createHash('md5');\n    hash.update(query + JSON.stringify(params));\n    return `query:${hash.digest('hex')}`;\n  }\n  \n  async execute(db, query, params, cacheTTL = this.ttl) {\n    const cacheKey = this.generateKey(query, params);\n    \n    // Check cache\n    const cached = await this.cache.get(cacheKey);\n    if (cached) {\n      return JSON.parse(cached);\n    }\n    \n    // Execute query\n    const result = await db.query(query, params);\n    \n    // Cache result\n    if (result.rows.length > 0) {\n      await this.cache.setex(\n        cacheKey, \n        cacheTTL, \n        JSON.stringify(result.rows)\n      );\n    }\n    \n    return result.rows;\n  }\n}\n</code></pre>\n<h2>Monitoring and Maintenance</h2>\n<h3>Performance Monitoring</h3>\n<pre><code class=\"language-sql\">-- Create monitoring views\nCREATE VIEW slow_queries AS\nSELECT \n    query,\n    calls,\n    total_time,\n    mean_time,\n    stddev_time,\n    rows,\n    100.0 * shared_blks_hit / nullif(shared_blks_hit + shared_blks_read, 0) AS hit_percent\nFROM pg_stat_statements \nWHERE calls > 100\nORDER BY mean_time DESC;\n\n-- Index usage statistics\nCREATE VIEW index_usage AS\nSELECT \n    schemaname,\n    tablename,\n    indexname,\n    idx_tup_read,\n    idx_tup_fetch,\n    idx_scan,\n    CASE \n        WHEN idx_scan = 0 THEN 'Never used'\n        WHEN idx_scan &#x3C; 100 THEN 'Rarely used'\n        ELSE 'Frequently used'\n    END as usage_level\nFROM pg_stat_user_indexes\nORDER BY idx_scan DESC;\n</code></pre>\n<h3>Automated Maintenance</h3>\n<pre><code class=\"language-sql\">-- Automated VACUUM and ANALYZE\nCREATE OR REPLACE FUNCTION auto_maintenance()\nRETURNS void AS $$\nDECLARE\n    table_record RECORD;\nBEGIN\n    -- Auto-vacuum tables with high update/delete activity\n    FOR table_record IN \n        SELECT schemaname, tablename, n_tup_upd, n_tup_del\n        FROM pg_stat_user_tables \n        WHERE n_tup_upd + n_tup_del > 1000\n    LOOP\n        EXECUTE format('VACUUM ANALYZE %I.%I', \n                      table_record.schemaname, \n                      table_record.tablename);\n        \n        RAISE NOTICE 'Vacuumed table %.%', \n                     table_record.schemaname, \n                     table_record.tablename;\n    END LOOP;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Schedule maintenance (requires pg_cron extension)\nSELECT cron.schedule('auto-maintenance', '0 2 * * *', 'SELECT auto_maintenance();');\n</code></pre>\n<h3>Health Check Queries</h3>\n<pre><code class=\"language-javascript\">// Database health monitoring\nclass DatabaseHealth {\n  constructor(db) {\n    this.db = db;\n  }\n  \n  async checkHealth() {\n    const checks = await Promise.allSettled([\n      this.checkConnectivity(),\n      this.checkSlowQueries(),\n      this.checkIndexUsage(),\n      this.checkTableBloat(),\n      this.checkReplicationLag()\n    ]);\n    \n    return {\n      timestamp: new Date().toISOString(),\n      checks: checks.map((result, index) => ({\n        name: ['connectivity', 'slow_queries', 'index_usage', 'table_bloat', 'replication_lag'][index],\n        status: result.status,\n        data: result.value || result.reason\n      }))\n    };\n  }\n  \n  async checkConnectivity() {\n    const result = await this.db.query('SELECT NOW() as current_time');\n    return { healthy: true, response_time: Date.now() };\n  }\n  \n  async checkSlowQueries() {\n    const result = await this.db.query(`\n      SELECT COUNT(*) as slow_query_count\n      FROM pg_stat_statements \n      WHERE mean_time > 1000 AND calls > 10\n    `);\n    \n    const count = parseInt(result.rows[0].slow_query_count);\n    return {\n      healthy: count &#x3C; 10,\n      slow_query_count: count\n    };\n  }\n  \n  async checkIndexUsage() {\n    const result = await this.db.query(`\n      SELECT COUNT(*) as unused_indexes\n      FROM pg_stat_user_indexes \n      WHERE idx_scan = 0\n    `);\n    \n    const count = parseInt(result.rows[0].unused_indexes);\n    return {\n      healthy: count &#x3C; 5,\n      unused_indexes: count\n    };\n  }\n}\n</code></pre>\n<h2>Performance Testing</h2>\n<h3>Load Testing with Artillery</h3>\n<pre><code class=\"language-yaml\"># artillery-config.yml\nconfig:\n  target: 'http://localhost:3000'\n  phases:\n    - duration: 60\n      arrivalRate: 10\n      name: \"Warm up\"\n    - duration: 300\n      arrivalRate: 50\n      name: \"Sustained load\"\n    - duration: 60\n      arrivalRate: 100\n      name: \"Peak load\"\n  \nscenarios:\n    - name: \"Database operations\"\n      weight: 100\n      flow:\n        - get:\n            url: \"/api/users/{{ $randomInt(1, 1000) }}\"\n        - post:\n            url: \"/api/orders\"\n            json:\n              user_id: \"{{ $randomInt(1, 1000) }}\"\n              items: [{ id: 1, quantity: 2 }]\n        - get:\n            url: \"/api/orders?user_id={{ $randomInt(1, 1000) }}\"\n</code></pre>\n<h3>Database Benchmarking</h3>\n<pre><code class=\"language-bash\">#!/bin/bash\n# benchmark.sh - Database performance testing\n\n# Connection parameters\nDB_HOST=\"localhost\"\nDB_NAME=\"testdb\"\nDB_USER=\"testuser\"\n\n# Test concurrent connections\necho \"Testing concurrent connections...\"\npgbench -h $DB_HOST -d $DB_NAME -U $DB_USER -c 10 -j 2 -T 60 -S\n\n# Test read/write performance\necho \"Testing read/write performance...\"\npgbench -h $DB_HOST -d $DB_NAME -U $DB_USER -c 20 -j 4 -T 120\n\n# Custom test with complex queries\necho \"Testing complex queries...\"\npgbench -h $DB_HOST -d $DB_NAME -U $DB_USER -c 5 -j 1 -T 60 -f complex_queries.sql\n</code></pre>\n<h2>Conclusion</h2>\n<p>Database optimization is an ongoing process that requires:</p>\n<ol>\n<li><strong>Proper indexing strategy</strong>: Create indexes that support your query patterns</li>\n<li><strong>Query optimization</strong>: Write efficient SQL and avoid common anti-patterns</li>\n<li><strong>Connection pooling</strong>: Manage database connections efficiently</li>\n<li><strong>Schema design</strong>: Balance normalization with performance requirements</li>\n<li><strong>Caching</strong>: Implement multiple layers of caching</li>\n<li><strong>Monitoring</strong>: Continuously monitor performance and identify bottlenecks</li>\n<li><strong>Maintenance</strong>: Regular VACUUM, ANALYZE, and index maintenance</li>\n</ol>\n<p>Key performance improvements you can expect:</p>\n<ul>\n<li><strong>Proper indexing</strong>: 10-100x query performance improvement</li>\n<li><strong>Connection pooling</strong>: 50-90% reduction in connection overhead</li>\n<li><strong>Query optimization</strong>: 2-10x improvement in complex queries</li>\n<li><strong>Caching</strong>: 90-99% reduction in database load for repeated queries</li>\n</ul>\n<p>Remember: measure first, optimize second. Always profile your specific workload and measure the impact of optimizations.</p>\n<p>The complete examples and benchmarking scripts are available on <a href=\"https://github.com/aayushpathak/database-optimization-guide\">GitHub</a>.</p>\n",
    "author": "Aayush Pathak",
    "date": "2024-01-10",
    "readTime": 15,
    "category": "Database",
    "tags": [
      "PostgreSQL",
      "Performance",
      "Optimization",
      "SQL"
    ],
    "image": "/blog/images/database-optimization.jpg",
    "featured": true
  }
];

export const projects: ProjectPost[] = [
  {
    "slug": "justdev-tools",
    "title": "Untitled Project",
    "excerpt": "No description available.",
    "content": "```markdown\n---\ntitle: \"justdev.tools\"\nexcerpt: \"Privacy-first PWA with offline developer utilities. No accounts, no data leaks — everything runs locally on your device.\"\nimage: \"/images/justdev/justdev.png\"\ntechnologies: [\"PWA\", \"JavaScript\", \"Service Workers\", \"Web APIs\"]\ngithubUrl: \"https://github.com/aayushpathak/justdev.tools\"\nliveUrl: \"https://justdev.tools\"\ndate: \"2025\"\ncategory: \"Developer Tools\"\nfeatured: true\nslug: \"justdev-tools\"\n---\n\n# justdev.tools\n\n## Project Overview\n\nA **privacy-first Progressive Web App (PWA)** that provides fast, local developer utilities without accounts or server dependencies. Once installed, it works fully offline and ensures no data ever leaves your device.\n\n## Architecture\n\n### Core Components\n\n1. **Local Execution Engine**: Runs all utilities in-memory within the browser\n2. **Service Worker**: Enables offline caching and instant startup\n3. **UI Layer**: Fast, minimal interface optimized for keyboard-first workflows\n4. **Data Helpers**: JSON, YAML, CSV, XML, and encoding utilities\n5. **Media & Testing Tools**: Lightweight image, regex, and HTTP payload utilities\n\n## Performance Characteristics\n\n- **Launch Time**: Instant (pre-cached PWA)\n- **Latency**: Sub-10ms for most transformations\n- **Offline Support**: 100% functional after first load\n- **Footprint**: Lightweight, minimal resource usage\n\n## Key Features\n\n### Data Conversion & Transformation\n- JSON, YAML, XML, CSV\n- URL encode/decode, Base64, hex\n\n### Security Helpers\n- JWT and token decoding\n- Header inspection\n- Hashing and checksum generation\n\n### Text & Content Tools\n- Prettify, minify, diff, and case transforms\n- Slug and timestamp generation\n\n### Media & Testing\n- Quick QR/barcode generation\n- Basic image resize and optimization\n- Regex tester and HTTP payload builder\n\n## Privacy & Security\n\n- **No Accounts**: Use instantly without sign-in\n- **Local Execution**: All operations run in memory\n- **Zero Telemetry**: No analytics, fingerprinting, or data exfiltration\n- **Installable**: Behaves like a native app on desktop and mobile\n\n## Deployment\n\n### PWA Installation\n- **Desktop (Chrome/Edge/Brave)**: Use \"Install\" or \"Open as app\"\n- **Android (Chrome)**: Add to home screen\n- **iOS (Safari)**: Add to home screen via Share → \"Add to Home Screen\"\n\nOnce installed, it runs standalone with its own icon and window.\n\n## Developer Experience\n\n- **Keyboard-first**: Shortcuts for common actions\n- **Minimal State**: Paste → Transform → Move on\n- **Fast Feedback**: Immediate results for inputs\n\n## Monitoring & Observability\n\nSince all tools run locally, no external monitoring exists. Performance validation relies on:\n- Lighthouse audits\n- Local storage and cache checks\n- Manual feature validation\n\n## Future Roadmap\n\n- Additional offline utilities for text, data, and media\n- Enhanced regex and HTTP testing tools\n- More quality-of-life improvements while keeping app lightweight\n- User-driven utility expansion\n```",
    "contentHtml": "<pre><code class=\"language-markdown\">---\ntitle: \"justdev.tools\"\nexcerpt: \"Privacy-first PWA with offline developer utilities. No accounts, no data leaks — everything runs locally on your device.\"\nimage: \"/images/justdev/justdev.png\"\ntechnologies: [\"PWA\", \"JavaScript\", \"Service Workers\", \"Web APIs\"]\ngithubUrl: \"https://github.com/aayushpathak/justdev.tools\"\nliveUrl: \"https://justdev.tools\"\ndate: \"2025\"\ncategory: \"Developer Tools\"\nfeatured: true\nslug: \"justdev-tools\"\n---\n\n# justdev.tools\n\n## Project Overview\n\nA **privacy-first Progressive Web App (PWA)** that provides fast, local developer utilities without accounts or server dependencies. Once installed, it works fully offline and ensures no data ever leaves your device.\n\n## Architecture\n\n### Core Components\n\n1. **Local Execution Engine**: Runs all utilities in-memory within the browser\n2. **Service Worker**: Enables offline caching and instant startup\n3. **UI Layer**: Fast, minimal interface optimized for keyboard-first workflows\n4. **Data Helpers**: JSON, YAML, CSV, XML, and encoding utilities\n5. **Media &#x26; Testing Tools**: Lightweight image, regex, and HTTP payload utilities\n\n## Performance Characteristics\n\n- **Launch Time**: Instant (pre-cached PWA)\n- **Latency**: Sub-10ms for most transformations\n- **Offline Support**: 100% functional after first load\n- **Footprint**: Lightweight, minimal resource usage\n\n## Key Features\n\n### Data Conversion &#x26; Transformation\n- JSON, YAML, XML, CSV\n- URL encode/decode, Base64, hex\n\n### Security Helpers\n- JWT and token decoding\n- Header inspection\n- Hashing and checksum generation\n\n### Text &#x26; Content Tools\n- Prettify, minify, diff, and case transforms\n- Slug and timestamp generation\n\n### Media &#x26; Testing\n- Quick QR/barcode generation\n- Basic image resize and optimization\n- Regex tester and HTTP payload builder\n\n## Privacy &#x26; Security\n\n- **No Accounts**: Use instantly without sign-in\n- **Local Execution**: All operations run in memory\n- **Zero Telemetry**: No analytics, fingerprinting, or data exfiltration\n- **Installable**: Behaves like a native app on desktop and mobile\n\n## Deployment\n\n### PWA Installation\n- **Desktop (Chrome/Edge/Brave)**: Use \"Install\" or \"Open as app\"\n- **Android (Chrome)**: Add to home screen\n- **iOS (Safari)**: Add to home screen via Share → \"Add to Home Screen\"\n\nOnce installed, it runs standalone with its own icon and window.\n\n## Developer Experience\n\n- **Keyboard-first**: Shortcuts for common actions\n- **Minimal State**: Paste → Transform → Move on\n- **Fast Feedback**: Immediate results for inputs\n\n## Monitoring &#x26; Observability\n\nSince all tools run locally, no external monitoring exists. Performance validation relies on:\n- Lighthouse audits\n- Local storage and cache checks\n- Manual feature validation\n\n## Future Roadmap\n\n- Additional offline utilities for text, data, and media\n- Enhanced regex and HTTP testing tools\n- More quality-of-life improvements while keeping app lightweight\n- User-driven utility expansion\n</code></pre>\n",
    "image": "https://images.pexels.com/photos/11035380/pexels-photo-11035380.jpeg?auto=compress&cs=tinysrgb&w=600",
    "technologies": [],
    "githubUrl": "#",
    "liveUrl": "#",
    "date": "2025",
    "category": "Development",
    "featured": false
  },
  {
    "slug": "microservices-api-gateway",
    "title": "Microservices API Gateway",
    "excerpt": "Scalable API gateway with rate limiting, authentication, load balancing, and service discovery. Handles 50k+ requests per second with sub-100ms latency.",
    "content": "\n# Microservices API Gateway\n\n## Project Overview\n\nA high-performance API gateway designed to handle massive traffic loads while providing essential features like authentication, rate limiting, and service discovery.\n\n## Architecture\n\n### Core Components\n\n1. **Request Router**: Intelligent routing based on URL patterns\n2. **Load Balancer**: Distributes traffic across service instances\n3. **Auth Service**: JWT-based authentication and authorization\n4. **Rate Limiter**: Token bucket algorithm for traffic control\n5. **Service Discovery**: Automatic service registration and health checks\n\n## Performance Characteristics\n\n- **Throughput**: 50,000+ requests/second\n- **Latency**: Sub-100ms response times\n- **Availability**: 99.99% uptime SLA\n- **Scalability**: Horizontal scaling across multiple nodes\n\n## Key Features\n\n### Authentication & Authorization\n```python\n@app.middleware(\"http\")\nasync def auth_middleware(request: Request, call_next):\n    if request.url.path.startswith(\"/api/\"):\n        token = request.headers.get(\"Authorization\")\n        if not token or not validate_jwt(token):\n            return JSONResponse(\n                status_code=401,\n                content={\"error\": \"Unauthorized\"}\n            )\n    \n    response = await call_next(request)\n    return response\n```\n\n### Rate Limiting\n- **Per-user limits**: Configurable rate limits per API key\n- **Global limits**: System-wide traffic throttling\n- **Burst handling**: Token bucket algorithm allows traffic bursts\n\n### Load Balancing\n- **Round-robin**: Default load balancing strategy\n- **Least connections**: Route to least busy service\n- **Health-aware**: Automatic failover for unhealthy services\n\n## Service Discovery\n\nThe gateway automatically discovers and registers services:\n\n```python\nclass ServiceRegistry:\n    def __init__(self):\n        self.services = {}\n        self.health_checker = HealthChecker()\n    \n    async def register_service(self, name: str, instances: List[str]):\n        self.services[name] = instances\n        await self.health_checker.start_monitoring(name, instances)\n    \n    async def get_healthy_instance(self, service_name: str) -> str:\n        healthy_instances = await self.health_checker.get_healthy(service_name)\n        return self.load_balancer.select(healthy_instances)\n```\n\n## Monitoring & Observability\n\n### Metrics Collection\n- Request/response metrics\n- Service health status\n- Performance benchmarks\n- Error rates and patterns\n\n### Logging\nStructured logging with correlation IDs for request tracing:\n\n```python\nlogger.info(\n    \"Request processed\",\n    extra={\n        \"correlation_id\": request.headers.get(\"X-Correlation-ID\"),\n        \"service\": target_service,\n        \"duration_ms\": duration,\n        \"status_code\": response.status_code\n    }\n)\n```\n\n## Deployment\n\n### Kubernetes Deployment\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api-gateway\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: api-gateway\n  template:\n    metadata:\n      labels:\n        app: api-gateway\n    spec:\n      containers:\n      - name: gateway\n        image: api-gateway:latest\n        ports:\n        - containerPort: 8000\n        env:\n        - name: REDIS_URL\n          value: \"redis://redis:6379\"\n        resources:\n          requests:\n            memory: \"256Mi\"\n            cpu: \"250m\"\n          limits:\n            memory: \"512Mi\"\n            cpu: \"500m\"\n```\n\n## Security Features\n\n- **JWT Authentication**: Stateless token-based auth\n- **Rate Limiting**: DDoS protection\n- **CORS Handling**: Cross-origin request management\n- **Request Validation**: Input sanitization and validation\n\n## Performance Optimizations\n\n1. **Connection Pooling**: Reuse HTTP connections to backend services\n2. **Caching**: Redis-based response caching\n3. **Async Processing**: Non-blocking I/O operations\n4. **Circuit Breaker**: Fail-fast for unhealthy services\n\n## Future Roadmap\n\n- **GraphQL Support**: Native GraphQL gateway capabilities\n- **WebSocket Proxying**: Real-time connection handling\n- **Advanced Analytics**: ML-based traffic analysis\n- **Multi-region Deployment**: Global load balancing",
    "contentHtml": "<h1>Microservices API Gateway</h1>\n<h2>Project Overview</h2>\n<p>A high-performance API gateway designed to handle massive traffic loads while providing essential features like authentication, rate limiting, and service discovery.</p>\n<h2>Architecture</h2>\n<h3>Core Components</h3>\n<ol>\n<li><strong>Request Router</strong>: Intelligent routing based on URL patterns</li>\n<li><strong>Load Balancer</strong>: Distributes traffic across service instances</li>\n<li><strong>Auth Service</strong>: JWT-based authentication and authorization</li>\n<li><strong>Rate Limiter</strong>: Token bucket algorithm for traffic control</li>\n<li><strong>Service Discovery</strong>: Automatic service registration and health checks</li>\n</ol>\n<h2>Performance Characteristics</h2>\n<ul>\n<li><strong>Throughput</strong>: 50,000+ requests/second</li>\n<li><strong>Latency</strong>: Sub-100ms response times</li>\n<li><strong>Availability</strong>: 99.99% uptime SLA</li>\n<li><strong>Scalability</strong>: Horizontal scaling across multiple nodes</li>\n</ul>\n<h2>Key Features</h2>\n<h3>Authentication &#x26; Authorization</h3>\n<pre><code class=\"language-python\">@app.middleware(\"http\")\nasync def auth_middleware(request: Request, call_next):\n    if request.url.path.startswith(\"/api/\"):\n        token = request.headers.get(\"Authorization\")\n        if not token or not validate_jwt(token):\n            return JSONResponse(\n                status_code=401,\n                content={\"error\": \"Unauthorized\"}\n            )\n    \n    response = await call_next(request)\n    return response\n</code></pre>\n<h3>Rate Limiting</h3>\n<ul>\n<li><strong>Per-user limits</strong>: Configurable rate limits per API key</li>\n<li><strong>Global limits</strong>: System-wide traffic throttling</li>\n<li><strong>Burst handling</strong>: Token bucket algorithm allows traffic bursts</li>\n</ul>\n<h3>Load Balancing</h3>\n<ul>\n<li><strong>Round-robin</strong>: Default load balancing strategy</li>\n<li><strong>Least connections</strong>: Route to least busy service</li>\n<li><strong>Health-aware</strong>: Automatic failover for unhealthy services</li>\n</ul>\n<h2>Service Discovery</h2>\n<p>The gateway automatically discovers and registers services:</p>\n<pre><code class=\"language-python\">class ServiceRegistry:\n    def __init__(self):\n        self.services = {}\n        self.health_checker = HealthChecker()\n    \n    async def register_service(self, name: str, instances: List[str]):\n        self.services[name] = instances\n        await self.health_checker.start_monitoring(name, instances)\n    \n    async def get_healthy_instance(self, service_name: str) -> str:\n        healthy_instances = await self.health_checker.get_healthy(service_name)\n        return self.load_balancer.select(healthy_instances)\n</code></pre>\n<h2>Monitoring &#x26; Observability</h2>\n<h3>Metrics Collection</h3>\n<ul>\n<li>Request/response metrics</li>\n<li>Service health status</li>\n<li>Performance benchmarks</li>\n<li>Error rates and patterns</li>\n</ul>\n<h3>Logging</h3>\n<p>Structured logging with correlation IDs for request tracing:</p>\n<pre><code class=\"language-python\">logger.info(\n    \"Request processed\",\n    extra={\n        \"correlation_id\": request.headers.get(\"X-Correlation-ID\"),\n        \"service\": target_service,\n        \"duration_ms\": duration,\n        \"status_code\": response.status_code\n    }\n)\n</code></pre>\n<h2>Deployment</h2>\n<h3>Kubernetes Deployment</h3>\n<pre><code class=\"language-yaml\">apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api-gateway\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: api-gateway\n  template:\n    metadata:\n      labels:\n        app: api-gateway\n    spec:\n      containers:\n      - name: gateway\n        image: api-gateway:latest\n        ports:\n        - containerPort: 8000\n        env:\n        - name: REDIS_URL\n          value: \"redis://redis:6379\"\n        resources:\n          requests:\n            memory: \"256Mi\"\n            cpu: \"250m\"\n          limits:\n            memory: \"512Mi\"\n            cpu: \"500m\"\n</code></pre>\n<h2>Security Features</h2>\n<ul>\n<li><strong>JWT Authentication</strong>: Stateless token-based auth</li>\n<li><strong>Rate Limiting</strong>: DDoS protection</li>\n<li><strong>CORS Handling</strong>: Cross-origin request management</li>\n<li><strong>Request Validation</strong>: Input sanitization and validation</li>\n</ul>\n<h2>Performance Optimizations</h2>\n<ol>\n<li><strong>Connection Pooling</strong>: Reuse HTTP connections to backend services</li>\n<li><strong>Caching</strong>: Redis-based response caching</li>\n<li><strong>Async Processing</strong>: Non-blocking I/O operations</li>\n<li><strong>Circuit Breaker</strong>: Fail-fast for unhealthy services</li>\n</ol>\n<h2>Future Roadmap</h2>\n<ul>\n<li><strong>GraphQL Support</strong>: Native GraphQL gateway capabilities</li>\n<li><strong>WebSocket Proxying</strong>: Real-time connection handling</li>\n<li><strong>Advanced Analytics</strong>: ML-based traffic analysis</li>\n<li><strong>Multi-region Deployment</strong>: Global load balancing</li>\n</ul>\n",
    "image": "https://images.pexels.com/photos/11035380/pexels-photo-11035380.jpeg?auto=compress&cs=tinysrgb&w=600",
    "technologies": [
      "Python",
      "FastAPI",
      "Kubernetes",
      "AWS",
      "MongoDB"
    ],
    "githubUrl": "https://github.com/aayushpathak/api-gateway",
    "liveUrl": "https://api-gateway.aayushpathak.com",
    "date": "2024",
    "category": "Microservices",
    "featured": true
  },
  {
    "slug": "distributed-task-queue-system",
    "title": "Distributed Task Queue System",
    "excerpt": "High-performance task queue system built with Redis and Node.js, capable of processing 100k+ jobs per minute with automatic retry logic and dead letter queues.",
    "content": "\n# Distributed Task Queue System\n\n## Overview\n\nThis project implements a high-performance distributed task queue system designed to handle massive workloads with reliability and scalability.\n\n## Architecture\n\nThe system consists of several key components:\n\n### Queue Manager\n- **Redis-based storage**: Leverages Redis for fast, persistent job storage\n- **Priority queues**: Support for multiple priority levels\n- **Dead letter queues**: Failed jobs are moved to DLQ for analysis\n\n### Worker Processes\n- **Horizontal scaling**: Add workers across multiple machines\n- **Graceful shutdown**: Workers complete current jobs before stopping\n- **Health monitoring**: Built-in health checks and metrics\n\n## Key Features\n\n### Performance\n- **100k+ jobs/minute**: Tested throughput under load\n- **Sub-second latency**: Jobs processed within milliseconds\n- **Memory efficient**: Optimized memory usage patterns\n\n### Reliability\n- **Automatic retries**: Configurable retry logic with exponential backoff\n- **Job persistence**: Jobs survive system restarts\n- **Monitoring**: Comprehensive metrics and alerting\n\n## Implementation Details\n\n```javascript\nconst TaskQueue = require('./lib/TaskQueue');\n\nconst queue = new TaskQueue({\n  redis: { host: 'localhost', port: 6379 },\n  concurrency: 10,\n  retries: 3\n});\n\n// Add a job\nawait queue.add('send-email', {\n  to: 'user@example.com',\n  subject: 'Welcome!',\n  template: 'welcome'\n});\n\n// Process jobs\nqueue.process('send-email', async (job) => {\n  const { to, subject, template } = job.data;\n  await emailService.send({ to, subject, template });\n});\n```\n\n## Performance Benchmarks\n\n- **Throughput**: 120,000 jobs/minute on 4-core machine\n- **Latency**: P99 < 50ms job processing time\n- **Memory**: <100MB for 1M queued jobs\n- **Reliability**: 99.99% job completion rate\n\n## Deployment\n\nThe system is containerized and can be deployed using Docker Compose or Kubernetes:\n\n```yaml\nversion: '3.8'\nservices:\n  queue-worker:\n    image: task-queue:latest\n    environment:\n      - REDIS_URL=redis://redis:6379\n      - WORKER_CONCURRENCY=10\n    depends_on:\n      - redis\n  \n  redis:\n    image: redis:7-alpine\n    volumes:\n      - redis_data:/data\n```\n\n## Monitoring\n\nBuilt-in Prometheus metrics provide visibility into:\n- Job processing rates\n- Queue depths\n- Error rates\n- Worker health\n\n## Future Enhancements\n\n- **Multi-tenant support**: Isolated queues per tenant\n- **Advanced scheduling**: Cron-like job scheduling\n- **Web UI**: Management dashboard for queue monitoring",
    "contentHtml": "<h1>Distributed Task Queue System</h1>\n<h2>Overview</h2>\n<p>This project implements a high-performance distributed task queue system designed to handle massive workloads with reliability and scalability.</p>\n<h2>Architecture</h2>\n<p>The system consists of several key components:</p>\n<h3>Queue Manager</h3>\n<ul>\n<li><strong>Redis-based storage</strong>: Leverages Redis for fast, persistent job storage</li>\n<li><strong>Priority queues</strong>: Support for multiple priority levels</li>\n<li><strong>Dead letter queues</strong>: Failed jobs are moved to DLQ for analysis</li>\n</ul>\n<h3>Worker Processes</h3>\n<ul>\n<li><strong>Horizontal scaling</strong>: Add workers across multiple machines</li>\n<li><strong>Graceful shutdown</strong>: Workers complete current jobs before stopping</li>\n<li><strong>Health monitoring</strong>: Built-in health checks and metrics</li>\n</ul>\n<h2>Key Features</h2>\n<h3>Performance</h3>\n<ul>\n<li><strong>100k+ jobs/minute</strong>: Tested throughput under load</li>\n<li><strong>Sub-second latency</strong>: Jobs processed within milliseconds</li>\n<li><strong>Memory efficient</strong>: Optimized memory usage patterns</li>\n</ul>\n<h3>Reliability</h3>\n<ul>\n<li><strong>Automatic retries</strong>: Configurable retry logic with exponential backoff</li>\n<li><strong>Job persistence</strong>: Jobs survive system restarts</li>\n<li><strong>Monitoring</strong>: Comprehensive metrics and alerting</li>\n</ul>\n<h2>Implementation Details</h2>\n<pre><code class=\"language-javascript\">const TaskQueue = require('./lib/TaskQueue');\n\nconst queue = new TaskQueue({\n  redis: { host: 'localhost', port: 6379 },\n  concurrency: 10,\n  retries: 3\n});\n\n// Add a job\nawait queue.add('send-email', {\n  to: 'user@example.com',\n  subject: 'Welcome!',\n  template: 'welcome'\n});\n\n// Process jobs\nqueue.process('send-email', async (job) => {\n  const { to, subject, template } = job.data;\n  await emailService.send({ to, subject, template });\n});\n</code></pre>\n<h2>Performance Benchmarks</h2>\n<ul>\n<li><strong>Throughput</strong>: 120,000 jobs/minute on 4-core machine</li>\n<li><strong>Latency</strong>: P99 &#x3C; 50ms job processing time</li>\n<li><strong>Memory</strong>: &#x3C;100MB for 1M queued jobs</li>\n<li><strong>Reliability</strong>: 99.99% job completion rate</li>\n</ul>\n<h2>Deployment</h2>\n<p>The system is containerized and can be deployed using Docker Compose or Kubernetes:</p>\n<pre><code class=\"language-yaml\">version: '3.8'\nservices:\n  queue-worker:\n    image: task-queue:latest\n    environment:\n      - REDIS_URL=redis://redis:6379\n      - WORKER_CONCURRENCY=10\n    depends_on:\n      - redis\n  \n  redis:\n    image: redis:7-alpine\n    volumes:\n      - redis_data:/data\n</code></pre>\n<h2>Monitoring</h2>\n<p>Built-in Prometheus metrics provide visibility into:</p>\n<ul>\n<li>Job processing rates</li>\n<li>Queue depths</li>\n<li>Error rates</li>\n<li>Worker health</li>\n</ul>\n<h2>Future Enhancements</h2>\n<ul>\n<li><strong>Multi-tenant support</strong>: Isolated queues per tenant</li>\n<li><strong>Advanced scheduling</strong>: Cron-like job scheduling</li>\n<li><strong>Web UI</strong>: Management dashboard for queue monitoring</li>\n</ul>\n",
    "image": "https://images.pexels.com/photos/325229/pexels-photo-325229.jpeg?auto=compress&cs=tinysrgb&w=600",
    "technologies": [
      "Node.js",
      "Redis",
      "Docker",
      "PostgreSQL",
      "TypeScript"
    ],
    "githubUrl": "https://github.com/aayushpathak/distributed-task-queue",
    "liveUrl": "https://task-queue-demo.aayushpathak.com",
    "date": "2024",
    "category": "Backend Systems",
    "featured": true
  }
];

export const generatedAt = "2025-09-13T17:05:15.471Z";
